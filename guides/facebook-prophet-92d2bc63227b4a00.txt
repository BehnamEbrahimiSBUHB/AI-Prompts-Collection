Directory structure:
└── _docs/
    ├── additional_topics.md
    ├── contributing.md
    ├── diagnostics.md
    ├── handling_shocks.md
    ├── holiday_effects.md
    ├── installation.md
    ├── multiplicative_seasonality.md
    ├── non-daily_data.md
    ├── outliers.md
    ├── quick_start.md
    ├── saturating_forecasts.md
    ├── seasonality,_holiday_effects,_and_regressors.md
    ├── trend_changepoints.md
    └── uncertainty_intervals.md

================================================
FILE: docs/_docs/additional_topics.md
================================================
---
layout: docs
docid: "additional_topics"
title: "Additional Topics"
permalink: /docs/additional_topics.html
subsections:
  - title: Saving models
    id: saving-models
  - title: Flat trend
    id: flat-trend
  - title: Custom trends
    id: custom-trends
  - title: Updating fitted models
    id: updating-fitted-models
  - title: minmax scaling (new in 1.1.5)
    id: minmax-scaling-(new-in-1.1.5)
  - title: Inspecting transformed data (new in 1.1.5)
    id: inspecting-transformed-data-(new-in-1.1.5)
  - title: External references
    id: external-references
---
<a id="saving-models"> </a>

### Saving models



It is possible to save fitted Prophet models so that they can be loaded and used later.



In R, this is done with `saveRDS` and `readRDS`:


```R
# R
saveRDS(m, file="model.RDS")  # Save model
m <- readRDS(file="model.RDS")  # Load model
```
In Python, models should not be saved with pickle; the Stan backend attached to the model object will not pickle well, and will produce issues under certain versions of Python. Instead, you should use the built-in serialization functions to serialize the model to json:


```python
# Python
from prophet.serialize import model_to_json, model_from_json

with open('serialized_model.json', 'w') as fout:
    fout.write(model_to_json(m))  # Save model

with open('serialized_model.json', 'r') as fin:
    m = model_from_json(fin.read())  # Load model
```
The json file will be portable across systems, and deserialization is backwards compatible with older versions of prophet.


<a id="flat-trend"> </a>

### Flat trend



For time series that exhibit strong seasonality patterns rather than trend changes, or when we want to rely on the pattern of exogenous regressors (e.g. for causal inference with time series), it may be useful to force the trend growth rate to be flat. This can be achieved simply by passing `growth='flat'` when creating the model:


```R
# R
m <- prophet(df, growth='flat')
```
```python
# Python
m = Prophet(growth='flat')
```
Below is a comparison of counterfactual forecasting with exogenous regressors using linear versus flat growth.


```python
# Python
import pandas as pd
from prophet import Prophet
import matplotlib.pyplot as plt

import warnings
warnings.simplefilter(action='ignore', category=FutureWarning)

regressor = "location_4"
target = "location_41"
cutoff = pd.to_datetime("2023-04-17 00:00:00")

df = (
    pd.read_csv(
        "https://raw.githubusercontent.com/facebook/prophet/main/examples/example_pedestrians_multivariate.csv", 
        parse_dates=["ds"]
    )
    .rename(columns={target: "y"})
)
train = df.loc[df["ds"] < cutoff]
test = df.loc[df["ds"] >= cutoff]
```
```python
# Python
def fit_model(growth):
    m = Prophet(growth=growth, seasonality_mode="multiplicative", daily_seasonality=15)
    m.add_regressor("location_4", mode="multiplicative")
    m.fit(train)
    preds = pd.merge(
        test,
        m.predict(test),
        on="ds",
        how="inner"
    )
    mape = ((preds["yhat"] - preds["y"]).abs() / preds_linear["y"]).mean()
    return m, preds, mape
```
```python
# Python
m_linear, preds_linear, mape_linear = fit_model("linear")
```
    01:19:58 - cmdstanpy - INFO - Chain [1] start processing
    01:19:58 - cmdstanpy - INFO - Chain [1] done processing


```python
# Python
m_flat, preds_flat, mape_flat = fit_model("flat")
```
    01:19:58 - cmdstanpy - INFO - Chain [1] start processing
    01:19:58 - cmdstanpy - INFO - Chain [1] done processing


```python
# Python
m_linear.plot_components(preds_linear);
```

![png](/prophet/static/additional_topics_files/additional_topics_16_0.png)


```python
# Python
m_flat.plot_components(preds_flat);
```

![png](/prophet/static/additional_topics_files/additional_topics_17_0.png)


```python
# Python
fig, ax = plt.subplots(figsize=(11, 5))
ax.scatter(preds_linear["ds"], preds_linear["y"], color="black", marker=".")
ax.plot(preds_linear["ds"], preds_linear["yhat"], label=f"linear, mape={mape_linear:.1%}")
ax.plot(preds_flat["ds"], preds_flat["yhat"], label=f"flat, mape={mape_flat:.1%}")
plt.xticks(rotation=60)
ax.legend();
```

![png](/prophet/static/additional_topics_files/additional_topics_18_0.png)


In this example, the target sensor values can be mostly explained by the exogenous regressor (a nearby sensor). The model with linear growth assumes an increasing trend and this leads to larger and larger over-predictions in the test period, while the flat growth model mostly follows movements in the exogenous regressor and this results in a sizeable MAPE improvement.



Note that forecasting with exogenous regressors is only effective when we can be confident in the future values of the regressor. The example above is relevant to causal inference using time series, where we want to understand what `y` would have looked like for a past time period, and hence the exogenous regressor values are known.



In other cases -- where we don't have exogenous regressors or have to predict their future values -- if flat growth is used on a time series that doesn't have a constant trend, any trend will be fit with the noise term and so there will be high predictive uncertainty in the forecast.


<a id="custom-trends"> </a>

### Custom trends



To use a trend besides these three built-in trend functions (piecewise linear, piecewise logistic growth, and flat), you can download the source code from github, modify the trend function as desired in a local branch, and then install that local version. [This PR](https://github.com/facebook/prophet/pull/1466/files) provides a good illustration of what must be done to implement a custom trend, as does [this one](https://github.com/facebook/prophet/pull/1794) that implements a step function trend and [this one](https://github.com/facebook/prophet/pull/1778) for a new trend in R.


<a id="updating-fitted-models"> </a>

### Updating fitted models



A common setting for forecasting is fitting models that need to be updated as additional data come in. Prophet models can only be fit once, and a new model must be re-fit when new data become available. In most settings, model fitting is fast enough that there isn't any issue with re-fitting from scratch. However, it is possible to speed things up a little by warm-starting the fit from the model parameters of the earlier model. This code example shows how this can be done in Python:


```python
# Python
def warm_start_params(m):
    """
    Retrieve parameters from a trained model in the format used to initialize a new Stan model.
    Note that the new Stan model must have these same settings:
        n_changepoints, seasonality features, mcmc sampling
    for the retrieved parameters to be valid for the new model.

    Parameters
    ----------
    m: A trained model of the Prophet class.

    Returns
    -------
    A Dictionary containing retrieved parameters of m.
    """
    res = {}
    for pname in ['k', 'm', 'sigma_obs']:
        if m.mcmc_samples == 0:
            res[pname] = m.params[pname][0][0]
        else:
            res[pname] = np.mean(m.params[pname])
    for pname in ['delta', 'beta']:
        if m.mcmc_samples == 0:
            res[pname] = m.params[pname][0]
        else:
            res[pname] = np.mean(m.params[pname], axis=0)
    return res

df = pd.read_csv('https://raw.githubusercontent.com/facebook/prophet/main/examples/example_wp_log_peyton_manning.csv')
df1 = df.loc[df['ds'] < '2016-01-19', :]  # All data except the last day
m1 = Prophet().fit(df1) # A model fit to all data except the last day


%timeit m2 = Prophet().fit(df)  # Adding the last day, fitting from scratch
%timeit m2 = Prophet().fit(df, init=warm_start_params(m1))  # Adding the last day, warm-starting from m1
```
    1.33 s ± 55.9 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)
    185 ms ± 4.46 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)


As can be seen, the parameters from the previous model are passed in to the fitting for the next with the kwarg `init`. In this case, model fitting was about 5x faster when using warm starting. The speedup will generally depend on how much the optimal model parameters have changed with the addition of the new data.



There are few caveats that should be kept in mind when considering warm-starting. First, warm-starting may work well for small updates to the data (like the addition of one day in the example above) but can be worse than fitting from scratch if there are large changes to the data (i.e., a lot of days have been added). This is because when a large amount of history is added, the location of the changepoints will be very different between the two models, and so the parameters from the previous model may actually produce a bad trend initialization. Second, as a detail, the number of changepoints need to be consistent from one model to the next or else an error will be raised because the changepoint prior parameter `delta` will be the wrong size.


<a id="minmax-scaling-(new-in-1.1.5)"> </a>

### minmax scaling (new in 1.1.5)



Before model fitting, Prophet scales `y` by dividing by the maximum value in the history. For datasets with very large `y` values, the scaled `y` values may be compressed to a very small range (i.e. `[0.99999... - 1.0]`), which causes a bad fit. This can be fixed by setting `scaling='minmax'` in the Prophet constructor.


```python
# Python
large_y = pd.read_csv(
    "https://raw.githubusercontent.com/facebook/prophet/main/python/prophet/tests/data3.csv", 
    parse_dates=["ds"]
)
```
```python
# Python
m1 = Prophet(scaling="absmax")
m1 = m1.fit(large_y)
```
    08:11:23 - cmdstanpy - INFO - Chain [1] start processing
    08:11:23 - cmdstanpy - INFO - Chain [1] done processing


```python
# Python
m2 = Prophet(scaling="minmax")
m2 = m2.fit(large_y)
```
    08:11:29 - cmdstanpy - INFO - Chain [1] start processing
    08:11:29 - cmdstanpy - INFO - Chain [1] done processing


```python
# Python
m1.plot(m1.predict(large_y));
```

![png](/prophet/static/additional_topics_files/additional_topics_28_0.png)


```python
# Python
m2.plot(m2.predict(large_y));
```

![png](/prophet/static/additional_topics_files/additional_topics_29_0.png)


<a id="inspecting-transformed-data-(new-in-1.1.5)"> </a>

### Inspecting transformed data (new in 1.1.5)



For debugging, it's useful to understand how the raw data has been transformed before being passed to the stan fit routine. We can call the `.preprocess()` method to see all the inputs to stan, and `.calculate_init_params()` to see how the model parameters will be initialized.


```python
# Python
df = pd.read_csv('https://raw.githubusercontent.com/facebook/prophet/main/examples/example_wp_log_peyton_manning.csv')
m = Prophet()
transformed = m.preprocess(df)
```
```python
# Python
transformed.y.head(n=10)
```



    0    0.746552
    1    0.663171
    2    0.637023
    3    0.628367
    4    0.614441
    5    0.605884
    6    0.654956
    7    0.687273
    8    0.652501
    9    0.628148
    Name: y_scaled, dtype: float64



```python
# Python
transformed.X.head(n=10)
```



<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>yearly_delim_1</th>
      <th>yearly_delim_2</th>
      <th>yearly_delim_3</th>
      <th>yearly_delim_4</th>
      <th>yearly_delim_5</th>
      <th>yearly_delim_6</th>
      <th>yearly_delim_7</th>
      <th>yearly_delim_8</th>
      <th>yearly_delim_9</th>
      <th>yearly_delim_10</th>
      <th>...</th>
      <th>yearly_delim_17</th>
      <th>yearly_delim_18</th>
      <th>yearly_delim_19</th>
      <th>yearly_delim_20</th>
      <th>weekly_delim_1</th>
      <th>weekly_delim_2</th>
      <th>weekly_delim_3</th>
      <th>weekly_delim_4</th>
      <th>weekly_delim_5</th>
      <th>weekly_delim_6</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>-0.377462</td>
      <td>0.926025</td>
      <td>-0.699079</td>
      <td>0.715044</td>
      <td>-0.917267</td>
      <td>0.398272</td>
      <td>-0.999745</td>
      <td>0.022576</td>
      <td>-0.934311</td>
      <td>-0.356460</td>
      <td>...</td>
      <td>0.335276</td>
      <td>-0.942120</td>
      <td>0.666089</td>
      <td>-0.745872</td>
      <td>-4.338837e-01</td>
      <td>-0.900969</td>
      <td>7.818315e-01</td>
      <td>0.623490</td>
      <td>-9.749279e-01</td>
      <td>-0.222521</td>
    </tr>
    <tr>
      <th>1</th>
      <td>-0.361478</td>
      <td>0.932381</td>
      <td>-0.674069</td>
      <td>0.738668</td>
      <td>-0.895501</td>
      <td>0.445059</td>
      <td>-0.995827</td>
      <td>0.091261</td>
      <td>-0.961479</td>
      <td>-0.274879</td>
      <td>...</td>
      <td>0.185987</td>
      <td>-0.982552</td>
      <td>0.528581</td>
      <td>-0.848883</td>
      <td>-9.749279e-01</td>
      <td>-0.222521</td>
      <td>4.338837e-01</td>
      <td>-0.900969</td>
      <td>7.818315e-01</td>
      <td>0.623490</td>
    </tr>
    <tr>
      <th>2</th>
      <td>-0.345386</td>
      <td>0.938461</td>
      <td>-0.648262</td>
      <td>0.761418</td>
      <td>-0.871351</td>
      <td>0.490660</td>
      <td>-0.987196</td>
      <td>0.159513</td>
      <td>-0.981538</td>
      <td>-0.191266</td>
      <td>...</td>
      <td>0.032249</td>
      <td>-0.999480</td>
      <td>0.375470</td>
      <td>-0.926834</td>
      <td>-7.818315e-01</td>
      <td>0.623490</td>
      <td>-9.749279e-01</td>
      <td>-0.222521</td>
      <td>-4.338837e-01</td>
      <td>-0.900969</td>
    </tr>
    <tr>
      <th>3</th>
      <td>-0.329192</td>
      <td>0.944263</td>
      <td>-0.621687</td>
      <td>0.783266</td>
      <td>-0.844881</td>
      <td>0.534955</td>
      <td>-0.973892</td>
      <td>0.227011</td>
      <td>-0.994341</td>
      <td>-0.106239</td>
      <td>...</td>
      <td>-0.122261</td>
      <td>-0.992498</td>
      <td>0.211276</td>
      <td>-0.977426</td>
      <td>5.505235e-14</td>
      <td>1.000000</td>
      <td>1.101047e-13</td>
      <td>1.000000</td>
      <td>1.984146e-12</td>
      <td>1.000000</td>
    </tr>
    <tr>
      <th>4</th>
      <td>-0.312900</td>
      <td>0.949786</td>
      <td>-0.594376</td>
      <td>0.804187</td>
      <td>-0.816160</td>
      <td>0.577825</td>
      <td>-0.955979</td>
      <td>0.293434</td>
      <td>-0.999791</td>
      <td>-0.020426</td>
      <td>...</td>
      <td>-0.273845</td>
      <td>-0.961774</td>
      <td>0.040844</td>
      <td>-0.999166</td>
      <td>7.818315e-01</td>
      <td>0.623490</td>
      <td>9.749279e-01</td>
      <td>-0.222521</td>
      <td>4.338837e-01</td>
      <td>-0.900969</td>
    </tr>
    <tr>
      <th>5</th>
      <td>-0.296516</td>
      <td>0.955028</td>
      <td>-0.566362</td>
      <td>0.824157</td>
      <td>-0.785267</td>
      <td>0.619157</td>
      <td>-0.933542</td>
      <td>0.358468</td>
      <td>-0.997850</td>
      <td>0.065537</td>
      <td>...</td>
      <td>-0.418879</td>
      <td>-0.908042</td>
      <td>-0.130793</td>
      <td>-0.991410</td>
      <td>9.749279e-01</td>
      <td>-0.222521</td>
      <td>-4.338837e-01</td>
      <td>-0.900969</td>
      <td>-7.818315e-01</td>
      <td>0.623490</td>
    </tr>
    <tr>
      <th>6</th>
      <td>-0.280044</td>
      <td>0.959987</td>
      <td>-0.537677</td>
      <td>0.843151</td>
      <td>-0.752283</td>
      <td>0.658840</td>
      <td>-0.906686</td>
      <td>0.421806</td>
      <td>-0.988531</td>
      <td>0.151016</td>
      <td>...</td>
      <td>-0.553893</td>
      <td>-0.832588</td>
      <td>-0.298569</td>
      <td>-0.954388</td>
      <td>4.338837e-01</td>
      <td>-0.900969</td>
      <td>-7.818315e-01</td>
      <td>0.623490</td>
      <td>9.749279e-01</td>
      <td>-0.222521</td>
    </tr>
    <tr>
      <th>7</th>
      <td>-0.263489</td>
      <td>0.964662</td>
      <td>-0.508356</td>
      <td>0.861147</td>
      <td>-0.717295</td>
      <td>0.696769</td>
      <td>-0.875539</td>
      <td>0.483147</td>
      <td>-0.971904</td>
      <td>0.235379</td>
      <td>...</td>
      <td>-0.675656</td>
      <td>-0.737217</td>
      <td>-0.457531</td>
      <td>-0.889193</td>
      <td>-4.338837e-01</td>
      <td>-0.900969</td>
      <td>7.818315e-01</td>
      <td>0.623490</td>
      <td>-9.749279e-01</td>
      <td>-0.222521</td>
    </tr>
    <tr>
      <th>8</th>
      <td>-0.246857</td>
      <td>0.969052</td>
      <td>-0.478434</td>
      <td>0.878124</td>
      <td>-0.680398</td>
      <td>0.732843</td>
      <td>-0.840248</td>
      <td>0.542202</td>
      <td>-0.948090</td>
      <td>0.318001</td>
      <td>...</td>
      <td>-0.781257</td>
      <td>-0.624210</td>
      <td>-0.602988</td>
      <td>-0.797750</td>
      <td>-9.749279e-01</td>
      <td>-0.222521</td>
      <td>4.338837e-01</td>
      <td>-0.900969</td>
      <td>7.818315e-01</td>
      <td>0.623490</td>
    </tr>
    <tr>
      <th>9</th>
      <td>-0.230151</td>
      <td>0.973155</td>
      <td>-0.447945</td>
      <td>0.894061</td>
      <td>-0.641689</td>
      <td>0.766965</td>
      <td>-0.800980</td>
      <td>0.598691</td>
      <td>-0.917267</td>
      <td>0.398272</td>
      <td>...</td>
      <td>-0.868168</td>
      <td>-0.496271</td>
      <td>-0.730644</td>
      <td>-0.682758</td>
      <td>-7.818315e-01</td>
      <td>0.623490</td>
      <td>-9.749279e-01</td>
      <td>-0.222521</td>
      <td>-4.338837e-01</td>
      <td>-0.900969</td>
    </tr>
  </tbody>
</table>
<p>10 rows × 26 columns</p>
</div>



```python
# Python
m.calculate_initial_params(num_total_regressors=transformed.K)
```



    ModelParams(k=-0.05444079622224118, m=0.7465517318876905, delta=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
           0., 0., 0., 0., 0., 0., 0., 0.]), beta=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
           0., 0., 0., 0., 0., 0., 0., 0., 0.]), sigma_obs=1.0)



<a id="external-references"> </a>

### External references



As we discuss in our [2023 blog post on the state of Prophet](https://medium.com/@cuongduong_35162/facebook-prophet-in-2023-and-beyond-c5086151c138), we have no plans to further develop the underlying Prophet model. If you're looking for state-of-the-art forecasting accuracy, we recommend the following libraries:



* [`statsforecast`](https://github.com/Nixtla/statsforecast), and other packages from the Nixtla group such as [`hierarchicalforecast`](https://github.com/Nixtla/hierarchicalforecast) and [`neuralforecast`](https://github.com/Nixtla/neuralforecast).

* [`NeuralProphet`](https://neuralprophet.com/), a Prophet-style model implemented in PyTorch, to be more adaptable and extensible.



These github repositories provide examples of building on top of Prophet in ways that may be of broad interest:



* [forecastr](https://github.com/garethcull/forecastr): A web app that provides a UI for Prophet.




================================================
FILE: docs/_docs/contributing.md
================================================
---
layout: docs
docid: "contributing"
title: "Getting Help and Contributing"
permalink: /docs/contributing.html
---

-----

**2023 Update:** We discuss our plans for the future of Prophet in this blog post: [facebook/prophet in 2023 and beyond](https://medium.com/@cuongduong_35162/facebook-prophet-in-2023-and-beyond-c5086151c138)

-----
Prophet has a non-fixed release cycle but we will be making bugfixes in response to user feedback and adding features. Please let us know if you encounter a bug by [filing an issue](https://github.com/facebook/prophet/issues). Github issues is also the right place to ask questions about using Prophet.

We appreciate all contributions. If you are planning to contribute back bug-fixes, please do so without any further discussion.

If you plan to contribute new features or extensions to the core, please first open an issue and discuss the feature with us. Sending a pull request is fine too, but it will likely be merged more quickly if any design decisions are settled on beforehand in an issue.

We try to keep the R and Python versions feature identical, but new features can be implemented for each method in separate commits.

The following sections will describe how you can submit a pull request for adding enhancements, documentation changes or bug fixes to the codebase.

## 1. Forking the Prophet Repo

You will need your own fork to work on the code. Go to the [prophet project
page](https://github.com/facebook/prophet) and hit the ``Fork`` button. You will
want to clone your fork to your machine:

```
$ git clone https://github.com/your-user-name/prophet.git
$ cd prophet
$ git remote add upstream https://github.com/facebook/prophet.git
```
This creates the directory `prophet` and connects your repository to
the upstream (main project) prophet repository.

## 2. Creating an environment with dependencies

Before starting any development, you'll need to create an isolated prophet
development environment. This should contain the required dependencies.

### Python

- Install either Anaconda [anaconda](https://www.anaconda.com/download/) or [miniconda](https://conda.io/miniconda.html)
- Make sure your conda is up to date (``conda update conda``)
- ``cd`` to the *prophet* source directory that you have cloned

```bash
$ cd python

# with Anaconda
$ conda create -n prophet
$ conda activate prophet

# with venv
$ python3 -m venv prophet
$ source prophet/bin/activate
```

### R

Dependencies can be managed through [``Packrat``](https://rstudio.github.io/packrat/) or [``renv``](https://rstudio.github.io/renv/articles/renv.html).

For ``renv``, you must first initialise a new project local environment.
```R
> setwd("path/to/prophet/R") # set R subdirectory as working directory
> install.packages('renv')
> renv::init()
```

This should also install the dependencies listed in the DESCRIPTION automatically. Any new R packages can be installed as they are needed in the project.

You can save the state of the project:

```R
> renv::snapshot()
```

or load the environment:

```R
> renv::restore()
```

## 3. Building a development version of Prophet

The next step is to build and install the development version of prophet in the environment you have just created.

### Python

```bash
$ python -m pip install -e ".[dev,parallel]"
```

You should be able to import *prophet* from your locally built version:

```bash
$ python  # start an interpreter
>>> import prophet
>>> prophet.__version__
'1.1.2'  # whatever the current github version is
```

This will create the new environment, and not touch any of your existing environments,
nor any existing Python installation.

```bash
# to view your environments:
$ conda info -e

# to return to your root environment::
$ conda deactivate
```

See the full conda docs [here](http://conda.pydata.org/docs).

### R

From the terminal, ``cd`` to ``R`` subdirectory and run:
```bash
$ R CMD INSTALL .
```

This will build and install the local version of the prophet package. Then from the R console you can load the package: ``library(prophet)``.

## 4. Creating a branch

You want your main branch to reflect only production-ready code, so create a
feature branch for making your changes. For example:

```bash
$ git checkout -b new-feature
```

This changes your working directory to the new-feature branch.  Keep any
changes in this branch specific to one bug or feature so it is clear
what the branch brings to *prophet*. You can have many "new-features"
and switch in between them using the ``git checkout`` command.

To update this branch, you need to retrieve the changes from the main branch:

```bash
$ git fetch upstream
$ git rebase upstream/main
```

This will replay your commits on top of the latest *prophet* git `main`.  If this
leads to merge conflicts, you must resolve these before submitting your pull
request.  If you have uncommitted changes, you will need to ``git stash`` them
prior to updating.  This will effectively store your changes and they can be
reapplied after updating.


## 5. Testing with Continuous Integration

Adding tests is one of the most common requests after code is pushed to prophet. Therefore, it is worth getting in the habit of writing tests ahead of time so this is never an issue. Once your pull request is submitted, the Github Actions CI (continuous integration) service automatically triggers Python and R builds for Prophet and runs the tests. A pull-request will be considered for merging when you have an all ‘green’ build. If any tests are failing, then you will get a red ‘X’, where you can click through to see the individual failed tests.

### Python

Prophet uses the ``pytest`` package for running tests in Python and ``testthat`` package for testing in R. All tests should go into the tests subdirectory in either the Python or R folders.

The entire test suite can be run by typing:
```bash
$ python -m pytest prophet/tests/
```

### R


The entire test suite can be run from the R console by installing ``devtools``:

```R
> install.packages('devtools')
> devtools::test()
```

Alternatively the test suite can be also run from the terminal after ``cd`` to the ``tests`` directory
```bash
$ Rscript testthat.R
```

or for just running a single test script like ``test_diagnostics.R`` from the R console:

```R
> library(testthat)
> source('test_diagnostics.R')
```

## 6. Generating documentation

Most of the `doc` pages are generated from [Jupyter notebooks](http://jupyter.org/) in the [notebooks](https://github.com/facebook/prophet/tree/main/notebooks) directory at the base of the source tree.  Please make changes there and then rebuild the docs:

```bash
$ cd docs
$ make notebooks
```

Make sure you have installed [rpy2](https://rpy2.bitbucket.io/) so that the R code can be run as well.

In R, the documentation for the source code must also generated if new parameters are added or a new function is created. This is documented with ``roxygen``.

Run the command below before submitting a PR with any changes to the R code to update the function documentation:

```R
> devtools::document()
```

## 7. Committing your code

Keep style fixes to a separate commit to make your pull request more readable. Once you’ve made changes, you can see them by typing:

```bash
$ git status
```

If you have created a new file, it is not being tracked by git. Add it by typing:

```bash
$ git add path/to/file-to-be-added.py
```

Doing ‘git status’ again should give something like:

```bash
# On branch new-feature
#
#       modified:   /relative/path/to/file-you-added.py
#
```

Now you can commit your changes in your local repository:

```bash
$ git commit -m
```

## 8. Pushing your changes

When you want your changes to appear publicly on your GitHub page, push your forked feature branch’s commits:

```bash
$ git push origin new-feature
```

Here origin is the default name given to your remote repository on GitHub. You can see the remote repositories:

```bash
$ git remote -v
```

If you added the upstream repository as described above you will see something like:

```bash
origin  git@github.com:yourname/prophet.git (fetch)
origin  git@github.com:yourname/prophet.git (push)
upstream	https://github.com/facebook/prophet.git (fetch)
upstream	https://github.com/facebook/prophet.git (push)
```

Now your code is on GitHub, but it is not yet a part of the prophet project. For that to happen, a pull request needs to be submitted on GitHub.

## 9. Review your code

When you’re ready to ask for a code review, file a pull request. Before you do, once again make sure that you have followed all the guidelines outlined in this document regarding code style, tests, performance tests, and documentation. You should also double check your branch changes against the branch it was based on:

1. Navigate to your repository on GitHub – https://github.com/your-user-name/prophet
2. Click on Branches
3. Click on the Compare button for your feature branch
4. Select the base and compare branches, if necessary. This will be `main` and new-feature, respectively.


## 10. Making a pull request

If everything looks good, you are ready to make a pull request. A pull request is how code from a local repository becomes available to the GitHub community and can be reviewed and eventually merged into the `main` version. This pull request and its associated changes will eventually be committed to the `main` branch and available in the next release. To submit a pull request:

1. Navigate to your repository on GitHub
2. Click on the Pull Request button
3. You can then click on Commits and Files Changed to make sure everything looks okay one last time
4. Write a description of your changes in the Preview Discussion tab
5. Click Send Pull Request.

This request then goes to the repository maintainers, and they will review the code. If you need to make more changes, you can make them in your branch, add them to a new commit, push them to GitHub, and the pull request will be automatically updated. Pushing them to GitHub again is done by:

```bash
$ git push origin new-feature
```

This will automatically update your pull request with the latest code and restart the Continuous Integration tests.


## 11. Delete your merged branch

Once your feature branch is merged into ``upstream main``, you can delete your remote branch via the ``Delete branch`` option in the PR and the local copy by running:

```bash
$ git branch -d new-feature
```

## 12. PR checklist

* Write docstrings for any functions that are included.
* Test that the documentation builds correctly. See “Generating documentation”.
* Test your code.
  - Write new tests if needed. See "Testing with Continuous Integration"
  - Test the code using unittest. Running all tests takes a while, so feel free to only run the tests you think are needed based on your PR. CI will catch any failing tests.
* In R, you can also run ``devtools:check()`` for carrying out a number of automated checks all at once, for code problems, documentation, testing, package structure, vignettes etc. This will take a few minutes to run.
* Once you push your changes and make a PR, make sure you use an informative title which summarizes the changes you have made.
* If the PR addresses an issue, please reference it e.g. fixes #1234



================================================
FILE: docs/_docs/diagnostics.md
================================================
---
layout: docs
docid: "diagnostics"
title: "Diagnostics"
permalink: /docs/diagnostics.html
subsections:
  - title: Cross validation
    id: cross-validation
  - title: Parallelizing cross validation
    id: parallelizing-cross-validation
  - title: Hyperparameter tuning
    id: hyperparameter-tuning
---
<a id="cross-validation"> </a>

### Cross validation



Prophet includes functionality for time series cross validation to measure forecast error using historical data. This is done by selecting cutoff points in the history, and for each of them fitting the model using data only up to that cutoff point. We can then compare the forecasted values to the actual values. This figure illustrates a simulated historical forecast on the Peyton Manning dataset, where the model was fit to an initial history of 5 years, and a forecast was made on a one year horizon.



![png](/prophet/static/diagnostics_files/diagnostics_4_0.png)


[The Prophet paper](https://peerj.com/preprints/3190.pdf) gives further description of simulated historical forecasts.



This cross validation procedure can be done automatically for a range of historical cutoffs using the `cross_validation` function. We specify the forecast horizon (`horizon`), and then optionally the size of the initial training period (`initial`) and the spacing between cutoff dates (`period`). By default, the initial training period is set to three times the horizon, and cutoffs are made every half a horizon.



The output of `cross_validation` is a dataframe with the true values `y` and the out-of-sample forecast values `yhat`, at each simulated forecast date and for each cutoff date. In particular, a forecast is made for every observed point between `cutoff` and `cutoff + horizon`. This dataframe can then be used to compute error measures of `yhat` vs. `y`.



Here we do cross-validation to assess prediction performance on a horizon of 365 days, starting with 730 days of training data in the first cutoff and then making predictions every 180 days. On this 8 year time series, this corresponds to 11 total forecasts.


```R
# R
df.cv <- cross_validation(m, initial = 730, period = 180, horizon = 365, units = 'days')
head(df.cv)
```
```python
# Python
from prophet.diagnostics import cross_validation
df_cv = cross_validation(m, initial='730 days', period='180 days', horizon = '365 days')
```
```python
# Python
df_cv.head()
```



<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>ds</th>
      <th>yhat</th>
      <th>yhat_lower</th>
      <th>yhat_upper</th>
      <th>y</th>
      <th>cutoff</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>2010-02-16</td>
      <td>8.954582</td>
      <td>8.462876</td>
      <td>9.452305</td>
      <td>8.242493</td>
      <td>2010-02-15</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2010-02-17</td>
      <td>8.720932</td>
      <td>8.222682</td>
      <td>9.242788</td>
      <td>8.008033</td>
      <td>2010-02-15</td>
    </tr>
    <tr>
      <th>2</th>
      <td>2010-02-18</td>
      <td>8.604608</td>
      <td>8.066920</td>
      <td>9.144968</td>
      <td>8.045268</td>
      <td>2010-02-15</td>
    </tr>
    <tr>
      <th>3</th>
      <td>2010-02-19</td>
      <td>8.526379</td>
      <td>8.029189</td>
      <td>9.043045</td>
      <td>7.928766</td>
      <td>2010-02-15</td>
    </tr>
    <tr>
      <th>4</th>
      <td>2010-02-20</td>
      <td>8.268247</td>
      <td>7.749520</td>
      <td>8.741847</td>
      <td>7.745003</td>
      <td>2010-02-15</td>
    </tr>
  </tbody>
</table>
</div>



In R, the argument `units` must be a type accepted by `as.difftime`, which is weeks or shorter. In Python, the string for `initial`, `period`, and `horizon` should be in the format used by Pandas Timedelta, which accepts units of days or shorter.



Custom cutoffs can also be supplied as a list of dates to the `cutoffs` keyword in the `cross_validation` function in Python and R. For example, three cutoffs six months apart, would need to be passed to the `cutoffs` argument in a date format like:


```R
# R
cutoffs <- as.Date(c('2013-02-15', '2013-08-15', '2014-02-15'))
df.cv2 <- cross_validation(m, cutoffs = cutoffs, horizon = 365, units = 'days')
```
```python
# Python
cutoffs = pd.to_datetime(['2013-02-15', '2013-08-15', '2014-02-15'])
df_cv2 = cross_validation(m, cutoffs=cutoffs, horizon='365 days')
```
The `performance_metrics` utility can be used to compute some useful statistics of the prediction performance (`yhat`, `yhat_lower`, and `yhat_upper` compared to `y`), as a function of the distance from the cutoff (how far into the future the prediction was). The statistics computed are mean squared error (MSE), root mean squared error (RMSE), mean absolute error (MAE), mean absolute percent error (MAPE), median absolute percent error (MDAPE) and coverage of the `yhat_lower` and `yhat_upper` estimates. These are computed on a rolling window of the predictions in `df_cv` after sorting by horizon (`ds` minus `cutoff`). By default 10% of the predictions will be included in each window, but this can be changed with the `rolling_window` argument.



In Python, you can also create custom performance metric using the `register_performance_metric` decorator. Created metric should contain following arguments:

 - df: Cross-validation results dataframe.

 - w: Aggregation window size.

 

and return:

- Dataframe with columns horizon and metric.


```R
# R
df.p <- performance_metrics(df.cv)
head(df.p)
```
```python
# Python
from prophet.diagnostics import performance_metrics
df_p = performance_metrics(df_cv)
df_p.head()
```



<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>horizon</th>
      <th>mse</th>
      <th>rmse</th>
      <th>mae</th>
      <th>mape</th>
      <th>mdape</th>
      <th>smape</th>
      <th>coverage</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>37 days</td>
      <td>0.493358</td>
      <td>0.702395</td>
      <td>0.503977</td>
      <td>0.058376</td>
      <td>0.049365</td>
      <td>0.058677</td>
      <td>0.676565</td>
    </tr>
    <tr>
      <th>1</th>
      <td>38 days</td>
      <td>0.499112</td>
      <td>0.706478</td>
      <td>0.508946</td>
      <td>0.058951</td>
      <td>0.049135</td>
      <td>0.059312</td>
      <td>0.675423</td>
    </tr>
    <tr>
      <th>2</th>
      <td>39 days</td>
      <td>0.521344</td>
      <td>0.722042</td>
      <td>0.515016</td>
      <td>0.059547</td>
      <td>0.049225</td>
      <td>0.060034</td>
      <td>0.672682</td>
    </tr>
    <tr>
      <th>3</th>
      <td>40 days</td>
      <td>0.528651</td>
      <td>0.727084</td>
      <td>0.517873</td>
      <td>0.059852</td>
      <td>0.049072</td>
      <td>0.060409</td>
      <td>0.676336</td>
    </tr>
    <tr>
      <th>4</th>
      <td>41 days</td>
      <td>0.536149</td>
      <td>0.732222</td>
      <td>0.518843</td>
      <td>0.059927</td>
      <td>0.049135</td>
      <td>0.060548</td>
      <td>0.681361</td>
    </tr>
  </tbody>
</table>
</div>



```python
# Python
from prophet.diagnostics import register_performance_metric, rolling_mean_by_h
import numpy as np
@register_performance_metric
def mase(df, w):
    """Mean absolute scale error

        Parameters
        ----------
        df: Cross-validation results dataframe.
        w: Aggregation window size.

        Returns
        -------
        Dataframe with columns horizon and mase.
    """
    e = (df['y'] - df['yhat'])
    d = np.abs(np.diff(df['y'])).sum()/(df['y'].shape[0]-1)
    se = np.abs(e/d)
    if w < 0:
        return pd.DataFrame({'horizon': df['horizon'], 'mase': se})
    return rolling_mean_by_h(
        x=se.values, h=df['horizon'].values, w=w, name='mase'
    )

df_mase = performance_metrics(df_cv, metrics=['mase'])
df_mase.head()
```



<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>horizon</th>
      <th>mase</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>37 days</td>
      <td>0.522946</td>
    </tr>
    <tr>
      <th>1</th>
      <td>38 days</td>
      <td>0.528102</td>
    </tr>
    <tr>
      <th>2</th>
      <td>39 days</td>
      <td>0.534401</td>
    </tr>
    <tr>
      <th>3</th>
      <td>40 days</td>
      <td>0.537365</td>
    </tr>
    <tr>
      <th>4</th>
      <td>41 days</td>
      <td>0.538372</td>
    </tr>
  </tbody>
</table>
</div>



Cross validation performance metrics can be visualized with `plot_cross_validation_metric`, here shown for MAPE. Dots show the absolute percent error for each prediction in `df_cv`. The blue line shows the MAPE, where the mean is taken over a rolling window of the dots. We see for this forecast that errors around 5% are typical for predictions one month into the future, and that errors increase up to around 11% for predictions that are a year out.


```R
# R
plot_cross_validation_metric(df.cv, metric = 'mape')
```
```python
# Python
from prophet.plot import plot_cross_validation_metric
fig = plot_cross_validation_metric(df_cv, metric='mape')
```

![png](/prophet/static/diagnostics_files/diagnostics_18_0.png)


The size of the rolling window in the figure can be changed with the optional argument `rolling_window`, which specifies the proportion of forecasts to use in each rolling window. The default is 0.1, corresponding to 10% of rows from `df_cv` included in each window; increasing this will lead to a smoother average curve in the figure. The `initial` period should be long enough to capture all of the components of the model, in particular seasonalities and extra regressors: at least a year for yearly seasonality, at least a week for weekly seasonality, etc.




<a id="parallelizing-cross-validation"> </a>

### Parallelizing cross validation



Cross-validation can also be run in parallel mode in Python, by setting specifying the `parallel` keyword. Four modes are supported



* `parallel=None` (Default, no parallelization)

* `parallel="processes"`

* `parallel="threads"`

* `parallel="dask"`



For problems that aren't too big, we recommend using `parallel="processes"`. It will achieve the highest performance when the parallel cross validation can be done on a single machine. For large problems, a [Dask](https://dask.org) cluster can be used to do the cross validation on many machines. You will need to [install Dask](https://docs.dask.org/en/latest/install.html) separately, as it will not be installed with `prophet`.





```python

from dask.distributed import Client



client = Client()  # connect to the cluster

df_cv = cross_validation(m, initial='730 days', period='180 days', horizon='365 days',

                         parallel="dask")

```


<a id="hyperparameter-tuning"> </a>

### Hyperparameter tuning



Cross-validation can be used for tuning hyperparameters of the model, such as `changepoint_prior_scale` and `seasonality_prior_scale`. A Python example is given below, with a 4x4 grid of those two parameters, with parallelization over cutoffs. Here parameters are evaluated on RMSE averaged over a 30-day horizon, but different performance metrics may be appropriate for different problems.


```python
# Python
import itertools
import numpy as np
import pandas as pd

param_grid = {  
    'changepoint_prior_scale': [0.001, 0.01, 0.1, 0.5],
    'seasonality_prior_scale': [0.01, 0.1, 1.0, 10.0],
}

# Generate all combinations of parameters
all_params = [dict(zip(param_grid.keys(), v)) for v in itertools.product(*param_grid.values())]
rmses = []  # Store the RMSEs for each params here

# Use cross validation to evaluate all parameters
for params in all_params:
    m = Prophet(**params).fit(df)  # Fit model with given params
    df_cv = cross_validation(m, cutoffs=cutoffs, horizon='30 days', parallel="processes")
    df_p = performance_metrics(df_cv, rolling_window=1)
    rmses.append(df_p['rmse'].values[0])

# Find the best parameters
tuning_results = pd.DataFrame(all_params)
tuning_results['rmse'] = rmses
print(tuning_results)
```

<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>changepoint_prior_scale</th>
      <th>seasonality_prior_scale</th>
      <th>rmse</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.001</td>
      <td>0.01</td>
      <td>0.757694</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.001</td>
      <td>0.10</td>
      <td>0.743399</td>
    </tr>
    <tr>
      <th>2</th>​
      <td>0.001</td>
      <td>1.00</td>
      <td>0.753387</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.001</td>
      <td>10.00</td>
      <td>0.762890</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.010</td>
      <td>0.01</td>
      <td>0.542315</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.010</td>
      <td>0.10</td>
      <td>0.535546</td>
    </tr>
    <tr>
      <th>6</th>
      <td>0.010</td>
      <td>1.00</td>
      <td>0.527008</td>
    </tr>
    <tr>
      <th>7</th>
      <td>0.010</td>
      <td>10.00</td>
      <td>0.541544</td>
    </tr>
    <tr>
      <th>8</th>
      <td>0.100</td>
      <td>0.01</td>
      <td>0.524835</td>
    </tr>
    <tr>
      <th>9</th>
      <td>0.100</td>
      <td>0.10</td>
      <td>0.516061</td>
    </tr>
    <tr>
      <th>10</th>
      <td>0.100</td>
      <td>1.00</td>
      <td>0.521406</td>
    </tr>
    <tr>
      <th>11</th>
      <td>0.100</td>
      <td>10.00</td>
      <td>0.518580</td>
    </tr>
    <tr>
      <th>12</th>
      <td>0.500</td>
      <td>0.01</td>
      <td>0.532140</td>
    </tr>
    <tr>
      <th>13</th>
      <td>0.500</td>
      <td>0.10</td>
      <td>0.524668</td>
    </tr>
    <tr>
      <th>14</th>
      <td>0.500</td>
      <td>1.00</td>
      <td>0.521130</td>
    </tr>
    <tr>
      <th>15</th>
      <td>0.500</td>
      <td>10.00</td>
      <td>0.522980</td>
    </tr>
  </tbody>
</table>

```python
# Python
best_params = all_params[np.argmin(rmses)]
print(best_params)
```

  {'changepoint_prior_scale': 0.1, 'seasonality_prior_scale': 0.1}

Alternatively, parallelization could be done across parameter combinations by parallelizing the loop above.



The Prophet model has a number of input parameters that one might consider tuning. Here are some general recommendations for hyperparameter tuning that may be a good starting place.



**Parameters that can be tuned**

- `changepoint_prior_scale`: This is probably the most impactful parameter. It determines the flexibility of the trend, and in particular how much the trend changes at the trend changepoints. As described in this documentation, if it is too small, the trend will be underfit and variance that should have been modeled with trend changes will instead end up being handled with the noise term. If it is too large, the trend will overfit and in the most extreme case you can end up with the trend capturing yearly seasonality. The default of 0.05 works for many time series, but this could be tuned; a range of [0.001, 0.5] would likely be about right. Parameters like this (regularization penalties; this is effectively a lasso penalty) are often tuned on a log scale.



- `seasonality_prior_scale`: This parameter controls the flexibility of the seasonality. Similarly, a large value allows the seasonality to fit large fluctuations, a small value shrinks the magnitude of the seasonality. The default is 10., which applies basically no regularization. That is because we very rarely see overfitting here (there's inherent regularization with the fact that it is being modeled with a truncated Fourier series, so it's essentially low-pass filtered). A reasonable range for tuning it would probably be [0.01, 10]; when set to 0.01 you should find that the magnitude of seasonality is forced to be very small. This likely also makes sense on a log scale, since it is effectively an L2 penalty like in ridge regression.



- `holidays_prior_scale`: This controls flexibility to fit holiday effects. Similar to seasonality_prior_scale, it defaults to 10.0 which applies basically no regularization, since we usually have multiple observations of holidays and can do a good job of estimating their effects. This could also be tuned on a range of [0.01, 10] as with seasonality_prior_scale.



- `seasonality_mode`: Options are [`'additive'`, `'multiplicative'`]. Default is `'additive'`, but many business time series will have multiplicative seasonality. This is best identified just from looking at the time series and seeing if the magnitude of seasonal fluctuations grows with the magnitude of the time series (see the documentation here on multiplicative seasonality), but when that isn't possible, it could be tuned.



**Maybe tune?**

- `changepoint_range`: This is the proportion of the history in which the trend is allowed to change. This defaults to 0.8, 80% of the history, meaning the model will not fit any trend changes in the last 20% of the time series. This is fairly conservative, to avoid overfitting to trend changes at the very end of the time series where there isn't enough runway left to fit it well. With a human in the loop, this is something that can be identified pretty easily visually: one can pretty clearly see if the forecast is doing a bad job in the last 20%. In a fully-automated setting, it may be beneficial to be less conservative. It likely will not be possible to tune this parameter effectively with cross validation over cutoffs as described above. The ability of the model to generalize from a trend change in the last 10% of the time series will be hard to learn from looking at earlier cutoffs that may not have trend changes in the last 10%. So, this parameter is probably better not tuned, except perhaps over a large number of time series. In that setting, [0.8, 0.95] may be a reasonable range.



**Parameters that would likely not be tuned**

- `growth`: Options are 'linear' and 'logistic'. This likely will not be tuned; if there is a known saturating point and growth towards that point it will be included and the logistic trend will be used, otherwise it will be linear.



- `changepoints`: This is for manually specifying the locations of changepoints. None by default, which automatically places them.



- `n_changepoints`: This is the number of automatically placed changepoints. The default of 25 should be plenty to capture the trend changes in a typical time series (at least the type that Prophet would work well on anyway). Rather than increasing or decreasing the number of changepoints, it will likely be more effective to focus on increasing or decreasing the flexibility at those trend changes, which is done with `changepoint_prior_scale`.



- `yearly_seasonality`: By default ('auto') this will turn yearly seasonality on if there is a year of data, and off otherwise. Options are ['auto', True, False]. If there is more than a year of data, rather than trying to turn this off during HPO, it will likely be more effective to leave it on and turn down seasonal effects by tuning `seasonality_prior_scale`.



- `weekly_seasonality`: Same as for `yearly_seasonality`.



- `daily_seasonality`: Same as for `yearly_seasonality`.



- `holidays`: This is to pass in a dataframe of specified holidays. The holiday effects would be tuned with `holidays_prior_scale`.



- `mcmc_samples`: Whether or not MCMC is used will likely be determined by factors like the length of the time series and the importance of parameter uncertainty (these considerations are described in the documentation).



- `interval_width`: Prophet `predict` returns uncertainty intervals for each component, like `yhat_lower` and `yhat_upper` for the forecast `yhat`. These are computed as quantiles of the posterior predictive distribution, and `interval_width` specifies which quantiles to use. The default of 0.8 provides an 80% prediction interval. You could change that to 0.95 if you wanted a 95% interval. This will affect only the uncertainty interval, and will not change the forecast `yhat` at all and so does not need to be tuned.



- `uncertainty_samples`: The uncertainty intervals are computed as quantiles from the posterior predictive interval, and the posterior predictive interval is estimated with Monte Carlo sampling. This parameter is the number of samples to use (defaults to 1000). The running time for predict will be linear in this number. Making it smaller will increase the variance (Monte Carlo error) of the uncertainty interval, and making it larger will reduce that variance. So, if the uncertainty estimates seem jagged this could be increased to further smooth them out, but it likely will not need to be changed. As with `interval_width`, this parameter only affects the uncertainty intervals and changing it will not affect in any way the forecast `yhat`; it does not need to be tuned.




================================================
FILE: docs/_docs/handling_shocks.md
================================================
---
layout: docs
docid: "handling_shocks"
title: "Handling Shocks"
permalink: /docs/handling_shocks.html
subsections:
  - title: Case Study - Pedestrian Activity
    id: case-study---pedestrian-activity
  - title: Default model without any adjustments
    id: default-model-without-any-adjustments
  - title: Treating COVID-19 lockdowns as a one-off holidays
    id: treating-covid-19-lockdowns-as-a-one-off-holidays
  - title: Sense checking the trend
    id: sense-checking-the-trend
  - title: Changes in seasonality between pre- and post-COVID
    id: changes-in-seasonality-between-pre--and-post-covid
  - title: Further reading
    id: further-reading
---
```python
# Python
%matplotlib inline
from prophet import Prophet
import pandas as pd
from matplotlib import pyplot as plt
import logging
logging.getLogger('prophet').setLevel(logging.ERROR)
import warnings
warnings.filterwarnings("ignore")

plt.rcParams['figure.figsize'] = 9, 6
```
As a result of the lockdowns caused by the COVID-19 pandemic, many time series experienced "shocks" during 2020, e.g. spikes in media consumption (Netflix, YouTube), e-commerce transactions (Amazon, eBay), whilst attendance to in-person events declined dramatically.



Most of these time series would also maintain their new level for a period of time, subject to fluctuations driven by easing of lockdowns and/or vaccines.



Seasonal patterns could have also changed: for example, people may have consumed less media (in total hours) on weekdays compared to weekends before the COVID lockdowns, but during lockdowns weekday consumption could be much closer to weekend consumption.



In this page we'll explore some strategies for capturing these effects using Prophet's functionality:



1. Marking step changes / spikes due to COVID events as once-off.

2. Sustained changes in behaviour leading to trend and seasonality changes.


<a id="case-study---pedestrian-activity"> </a>

#### Case Study - Pedestrian Activity



For this case study we'll use [Pedestrian Sensor data from the City of Melbourne](https://data.melbourne.vic.gov.au/Transport/Pedestrian-Counting-System-Monthly-counts-per-hour/b2ak-trbp). This data measures foot traffic from sensors in various places in the central business district, and we've chosen one sensor (`Sensor_ID = 4`) and aggregated the values to a daily grain. 



The aggregated dataset can be found in the examples folder [here](https://github.com/facebook/prophet/tree/master/examples/example_pedestrians_covid.csv).


```python
# Python
df = pd.read_csv('https://raw.githubusercontent.com/facebook/prophet/main/examples/example_pedestrians_covid.csv')
```
```python
# Python
df.set_index('ds').plot();
```

![png](/prophet/static/handling_shocks_files/handling_shocks_4_0.png)


We can see two key events in the time series:



* The initial drop in foot traffic around 21 March 2020, which started to recover around 6 June 2020. This corresponds to the declaration of a pandemic by WHO and subsequent lockdowns mandated by the Victorian government.

* After some slow recovery, a second drop in foot traffic around July 9 2020, which began to recover around 27 October 2020. This corresponds to the "second wave" of the pandemic in metropolitan Melbourne.



There are also shorter periods of strict lockdown that lead to sudden tips in the time series: 5 days in February 2021, and 14 days in early June 2021.


<a id="default-model-without-any-adjustments"> </a>

#### Default model without any adjustments



First we'll fit a model with the default Prophet settings:


```python
# Python
m = Prophet()
m = m.fit(df)
future = m.make_future_dataframe(periods=366)
forecast = m.predict(future)
```
    02:53:41 - cmdstanpy - INFO - Chain [1] start processing
    02:53:41 - cmdstanpy - INFO - Chain [1] done processing


```python
# Python
m.plot(forecast)
plt.axhline(y=0, color='red')
plt.title('Default Prophet');
```

![png](/prophet/static/handling_shocks_files/handling_shocks_8_0.png)


```python
# Python
m.plot_components(forecast);
```

![png](/prophet/static/handling_shocks_files/handling_shocks_9_0.png)


The model seems to fit reasonably to past data, but notice how we're capturing the dips, and the spikes after the dips, as a part of the trend component. 



By default, the model assumes that these large spikes are possible in the future, even though we realistically won't see something of the same magnitude within our forecast horizon (1 year in this case). This leads to a fairly optimistic forecast of the recovery of foot traffic in 2022.


<a id="treating-covid-19-lockdowns-as-a-one-off-holidays"> </a>

### Treating COVID-19 lockdowns as a one-off holidays



To prevent large dips and spikes from being captured by the trend component, we can treat the days impacted by COVID-19 as holidays that will not repeat again in the future. Adding custom holidays is explained in more detail [here](https://facebook.github.io/prophet/docs/seasonality,_holiday_effects,_and_regressors.html#modeling-holidays-and-special-events). We set up a DataFrame like so to describe the periods affected by lockdowns:


```python
# Python
lockdowns = pd.DataFrame([
    {'holiday': 'lockdown_1', 'ds': '2020-03-21', 'lower_window': 0, 'ds_upper': '2020-06-06'},
    {'holiday': 'lockdown_2', 'ds': '2020-07-09', 'lower_window': 0, 'ds_upper': '2020-10-27'},
    {'holiday': 'lockdown_3', 'ds': '2021-02-13', 'lower_window': 0, 'ds_upper': '2021-02-17'},
    {'holiday': 'lockdown_4', 'ds': '2021-05-28', 'lower_window': 0, 'ds_upper': '2021-06-10'},
])
for t_col in ['ds', 'ds_upper']:
    lockdowns[t_col] = pd.to_datetime(lockdowns[t_col])
lockdowns['upper_window'] = (lockdowns['ds_upper'] - lockdowns['ds']).dt.days
lockdowns
```



<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>holiday</th>
      <th>ds</th>
      <th>lower_window</th>
      <th>ds_upper</th>
      <th>upper_window</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>lockdown_1</td>
      <td>2020-03-21</td>
      <td>0</td>
      <td>2020-06-06</td>
      <td>77</td>
    </tr>
    <tr>
      <th>1</th>
      <td>lockdown_2</td>
      <td>2020-07-09</td>
      <td>0</td>
      <td>2020-10-27</td>
      <td>110</td>
    </tr>
    <tr>
      <th>2</th>
      <td>lockdown_3</td>
      <td>2021-02-13</td>
      <td>0</td>
      <td>2021-02-17</td>
      <td>4</td>
    </tr>
    <tr>
      <th>3</th>
      <td>lockdown_4</td>
      <td>2021-05-28</td>
      <td>0</td>
      <td>2021-06-10</td>
      <td>13</td>
    </tr>
  </tbody>
</table>
</div>



* We have an entry for each lockdown period, with `ds` specifying the start of the lockdown. `ds_upper` is not used by Prophet, but it's a convenient way for us to calculate `upper_window`.

* `upper_window` tells Prophet that the lockdown spans for x days after the start of the lockdown. Note that the holidays regression is inclusive of the upper bound.



Note that since we don't specify any future dates, Prophet will assume that these holidays will _not_ occur again when creating the future dataframe (and hence they won't affect our projections). This is different to how we would specify a recurring holiday.


```python
# Python
m2 = Prophet(holidays=lockdowns)
m2 = m2.fit(df)
future2 = m2.make_future_dataframe(periods=366)
forecast2 = m2.predict(future2)
```
    02:53:44 - cmdstanpy - INFO - Chain [1] start processing
    02:53:45 - cmdstanpy - INFO - Chain [1] done processing


```python
# Python
m2.plot(forecast2)
plt.axhline(y=0, color='red')
plt.title('Lockdowns as one-off holidays');
```

![png](/prophet/static/handling_shocks_files/handling_shocks_15_0.png)


```python
# Python
m2.plot_components(forecast2);
```

![png](/prophet/static/handling_shocks_files/handling_shocks_16_0.png)


* Prophet is sensibly assigning a large negative effect to the days within the lockdown periods.

* The forecast for the trend is not as strong / optimistic and seems fairly reasonable.



<a id="sense-checking-the-trend"> </a>

### Sense checking the trend



In an environment when behaviours are constantly changing, it's important to ensure that the trend component of the model is capturing to emerging patterns without overfitting to them.



The [trend changepoints](https://facebook.github.io/prophet/docs/trend_changepoints.html) documentation explains two things we could tweak with the trend component:



* The changepoint locations, which by default are evenly spaced across 80% of the history. We should be mindful of where this range ends, and extend the range (either by increasing the % or adding changepoints manually) if we believe the most recent data better reflects future behaviour.

* The strength of regularisation (`changepoint_prior_scale`), which determines how flexible the trend is; the default value is `0.05` and increasing this will allow the trend to fit more closely to the observed data.



We plot the trend component and changepoints detected by our current model below.


```python
# Python
from prophet.plot import add_changepoints_to_plot
fig = m2.plot(forecast2)
a = add_changepoints_to_plot(fig.gca(), m2, forecast2)
```

![png](/prophet/static/handling_shocks_files/handling_shocks_18_0.png)


The detected changepoints look reasonable, and the future trend tracks the latest upwards trend in activity, but not to the extent of late 2020. This seems suitable for a best guess of future activity. 



We can see what the forecast would look like if we wanted to emphasize COVID patterns more in model training; we can do this by adding more potential changepoints after 2020 and making the trend more flexible.


```python
# Python
m3_changepoints = (
    # 10 potential changepoints in 2.5 years
    pd.date_range('2017-06-02', '2020-01-01', periods=10).date.tolist() + 
    # 15 potential changepoints in 1 year 2 months
    pd.date_range('2020-02-01', '2021-04-01', periods=15).date.tolist()
)
```
```python
# Python
# Default changepoint_prior_scale is 0.05, so 1.0 will lead to much more flexibility in comparison.
m3 = Prophet(holidays=lockdowns, changepoints=m3_changepoints, changepoint_prior_scale=1.0)
m3 = m3.fit(df)
forecast3 = m3.predict(future2)
```
    02:53:49 - cmdstanpy - INFO - Chain [1] start processing
    02:53:52 - cmdstanpy - INFO - Chain [1] done processing


```python
# Python
from prophet.plot import add_changepoints_to_plot
fig = m3.plot(forecast3)
a = add_changepoints_to_plot(fig.gca(), m3, forecast3)
```

![png](/prophet/static/handling_shocks_files/handling_shocks_22_0.png)


We're seeing many changepoints detected post-COVID, matching the various fluctuations from loosening / tightening lockdowns. Overall the trend curve and forecasted trend are quite similar to our previous model, but we're seeing a lot more uncertainty because of the higher number of trend changes we picked up in the history.



We probably wouldn't pick this model over the model with default parameters as a best estimate, but it's a good demonstration of how we can incorporate our beliefs into the model about which patterns are important to capture.


<a id="changes-in-seasonality-between-pre--and-post-covid"> </a>

### Changes in seasonality between pre- and post-COVID



The seasonal component plots in the previous sections show a peak of activity on Friday compared to other days of the week. If we're not sure whether this will still hold post-lockdown, we can add _conditional seasonalities_ to the model. Conditional seasonalities are explained in more detail [here](https://facebook.github.io/prophet/docs/seasonality,_holiday_effects,_and_regressors.html#seasonalities-that-depend-on-other-factors).



First we define boolean columns in the history dataframe to flag "pre covid" and "post covid" periods:


```python
# Python
df2 = df.copy()
df2['pre_covid'] = pd.to_datetime(df2['ds']) < pd.to_datetime('2020-03-21')
df2['post_covid'] = ~df2['pre_covid']
```
The conditional seasonality we're interested in modelling here is the day-of-week ("weekly") seasonality. To do this, we firstly turn off the default `weekly_seasonality` when we create the Prophet model.


```python
# Python
m4 = Prophet(holidays=lockdowns, weekly_seasonality=False)
```
We then add this weekly seasonality manually, as two different model components - one for pre-covid, one for post-covid. Note that `fourier_order=3` is the default setting for weekly seasonality. After this we can run `.fit()`.


```python
# Python
m4.add_seasonality(
    name='weekly_pre_covid',
    period=7,
    fourier_order=3,
    condition_name='pre_covid',
)
m4.add_seasonality(
    name='weekly_post_covid',
    period=7,
    fourier_order=3,
    condition_name='post_covid',
);
```
```python
# Python
m4 = m4.fit(df2)
```
    02:53:55 - cmdstanpy - INFO - Chain [1] start processing
    02:53:56 - cmdstanpy - INFO - Chain [1] done processing


We also need to create the `pre_covid` and `post_covid` flags in the future dataframe. This is so that Prophet can apply the correct weekly seasonality parameters to each future date.


```python
# Python
future4 = m4.make_future_dataframe(periods=366)
future4['pre_covid'] = pd.to_datetime(future4['ds']) < pd.to_datetime('2020-03-21')
future4['post_covid'] = ~future4['pre_covid']
```
```python
# Python
forecast4 = m4.predict(future4)
```
```python
# Python
m4.plot(forecast4)
plt.axhline(y=0, color='red')
plt.title('Lockdowns as one-off holidays + Conditional weekly seasonality');
```

![png](/prophet/static/handling_shocks_files/handling_shocks_34_0.png)


```python
# Python
m4.plot_components(forecast4);
```

![png](/prophet/static/handling_shocks_files/handling_shocks_35_0.png)


Interestingly, the model with conditional seasonalities suggests that, post-COVID, pedestrian activity peaks on Saturdays, instead of Fridays. This could make sense if most people are still working from home and are hence less likely to go out on Friday nights. From a prediction perspective this would only be important if we care about predicting weekdays vs. weekends accurately, but overall this kind of exploration helps us gain insight into how COVID has changed behaviours.


<a id="further-reading"> </a>

#### Further reading



A lot of the content in this page was inspired by this [GitHub discussion](https://github.com/facebook/prophet/issues/1416). We've covered a few low hanging fruits for tweaking Prophet models when faced with shocks such as COVID, but there are many other possible approaches as well, such as:



* Using external regressors (e.g. the lockdown stringency index). This would only be fruitful if we a) have regressor data that aligns well (in terms of location) with the series we're forecasting, and b) have control over or can predict the regressor much more accurately than the time series alone.

* Detecting and removing outlier data from the training period, or throwing away older training data completely. This might be a better approach for sub-daily time series that don't have yearly seasonal patterns.



Overall though it's difficult to be confident in our forecasts in these environments when rules are constantly changing and outbreaks occur randomly. In this scenario it's more important to constantly re-train / re-evaluate our models and clearly communicate the increased uncertainty in forecasts.


```python
# Python

```


================================================
FILE: docs/_docs/holiday_effects.md
================================================
---
layout: docs
docid: "holiday_effects"
title: "Holiday Effects"
permalink: /docs/holiday_effects.html
---
### Modeling Holidays
If you have holidays that you'd like to model, you must create a dataframe for them. It has two columns (`holiday` and `ds`) and a row for each occurrence of the holiday. It must include all occurrences of the holiday, both in the past (back as far as the historical data go) and in the future (out as far as the forecast is being made). If they won't repeat in the future, Prophet will model them and then not include them in the forecast.

You can also include columns `lower_window` and `upper_window` which extend the holiday out to `[lower_window, upper_window]` days around the date. For instance, if you wanted to included Christmas Eve in addition to Christmas you'd include `lower_window=-1,upper_window=0`. If you wanted to use Black Friday in addition to Thanksgiving, you'd include `lower_window=0,upper_window=1`.

Here we create a dataframe that includes the dates of all of Peyton Manning's playoff appearances:

```python
# Python
playoffs = pd.DataFrame({
  'holiday': 'playoff',
  'ds': pd.to_datetime(['2008-01-13', '2009-01-03', '2010-01-16',
                        '2010-01-24', '2010-02-07', '2011-01-08',
                        '2013-01-12', '2014-01-12', '2014-01-19',
                        '2014-02-02', '2015-01-11', '2016-01-17',
                        '2016-01-24', '2016-02-07']),
  'lower_window': 0,
  'upper_window': 1,
})
superbowls = pd.DataFrame({
  'holiday': 'superbowl',
  'ds': pd.to_datetime(['2010-02-07', '2014-02-02', '2016-02-07']),
  'lower_window': 0,
  'upper_window': 1,
})
holidays = pd.concat((playoffs, superbowls))
```
```R
# R
library(dplyr)
playoffs <- data_frame(
  holiday = 'playoff',
  ds = as.Date(c('2008-01-13', '2009-01-03', '2010-01-16',
                 '2010-01-24', '2010-02-07', '2011-01-08',
                 '2013-01-12', '2014-01-12', '2014-01-19',
                 '2014-02-02', '2015-01-11', '2016-01-17',
                 '2016-01-24', '2016-02-07')),
  lower_window = 0,
  upper_window = 1
)
superbowls <- data_frame(
  holiday = 'superbowl',
  ds = as.Date(c('2010-02-07', '2014-02-02', '2016-02-07')),
  lower_window = 0,
  upper_window = 1
)
holidays <- bind_rows(playoffs, superbowls)
```
Above we have include the superbowl days as both playoff games and superbowl games. This means that the superbowl effect will be an additional additive bonus on top of the playoff effect.

Once the table is created, holiday effects are included in the forecast by passing them in with the `holidays` argument. Here we do it with the Peyton Manning data from the Quickstart:

```python
# Python
m = Prophet(holidays=holidays)
forecast = m.fit(df).predict(future)
```
```R
# R
m <- prophet(df, holidays = holidays)
forecast <- predict(m, future)
```
The holiday effect can be seen in the `forecast` dataframe:

```R
# R
forecast %>% 
  select(ds, playoff, superbowl) %>% 
  filter(abs(playoff + superbowl) > 0) %>%
  tail(10)
```
```python
# Python
forecast[(forecast['playoff'] + forecast['superbowl']).abs() > 0][
        ['ds', 'playoff', 'superbowl']][-10:]
```



<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>ds</th>
      <th>playoff</th>
      <th>superbowl</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>2190</th>
      <td>2014-02-02</td>
      <td>1.220308</td>
      <td>1.204992</td>
    </tr>
    <tr>
      <th>2191</th>
      <td>2014-02-03</td>
      <td>1.900465</td>
      <td>1.444581</td>
    </tr>
    <tr>
      <th>2532</th>
      <td>2015-01-11</td>
      <td>1.220308</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>2533</th>
      <td>2015-01-12</td>
      <td>1.900465</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>2901</th>
      <td>2016-01-17</td>
      <td>1.220308</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>2902</th>
      <td>2016-01-18</td>
      <td>1.900465</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>2908</th>
      <td>2016-01-24</td>
      <td>1.220308</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>2909</th>
      <td>2016-01-25</td>
      <td>1.900465</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>2922</th>
      <td>2016-02-07</td>
      <td>1.220308</td>
      <td>1.204992</td>
    </tr>
    <tr>
      <th>2923</th>
      <td>2016-02-08</td>
      <td>1.900465</td>
      <td>1.444581</td>
    </tr>
  </tbody>
</table>
</div>



The holiday effects will also show up in the components plot, where we see that there is a spike on the days around playoff appearances, with an especially large spike for the superbowl:

```python
# Python
m.plot_components(forecast);
```
```R
# R
prophet_plot_components(m, forecast);
```
 
![png](/prophet/static/holiday_effects_files/holiday_effects_13_0.png) 


### Prior scale for holidays and seasonality
If you find that the holidays are overfitting, you can adjust their prior scale to smooth them using the parameter `holidays_prior_scale`, which by default is 10:

```R
# R
m <- prophet(df, holidays = holidays, holidays.prior.scale = 1)
forecast <- predict(m, future)
forecast %>% 
  select(ds, playoff, superbowl) %>% 
  filter(abs(playoff + superbowl) > 0) %>%
  tail(10)
```
```python
# Python
m = Prophet(holidays=holidays, holidays_prior_scale=1).fit(df)
forecast = m.predict(future)
forecast[(forecast['playoff'] + forecast['superbowl']).abs() > 0][
    ['ds', 'playoff', 'superbowl']][-10:]
```



<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>ds</th>
      <th>playoff</th>
      <th>superbowl</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>2190</th>
      <td>2014-02-02</td>
      <td>1.362312</td>
      <td>0.693425</td>
    </tr>
    <tr>
      <th>2191</th>
      <td>2014-02-03</td>
      <td>2.033471</td>
      <td>0.542254</td>
    </tr>
    <tr>
      <th>2532</th>
      <td>2015-01-11</td>
      <td>1.362312</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>2533</th>
      <td>2015-01-12</td>
      <td>2.033471</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>2901</th>
      <td>2016-01-17</td>
      <td>1.362312</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>2902</th>
      <td>2016-01-18</td>
      <td>2.033471</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>2908</th>
      <td>2016-01-24</td>
      <td>1.362312</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>2909</th>
      <td>2016-01-25</td>
      <td>2.033471</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>2922</th>
      <td>2016-02-07</td>
      <td>1.362312</td>
      <td>0.693425</td>
    </tr>
    <tr>
      <th>2923</th>
      <td>2016-02-08</td>
      <td>2.033471</td>
      <td>0.542254</td>
    </tr>
  </tbody>
</table>
</div>



The magnitude of the holiday effect has been reduced compared to before, especially for superbowls, which had the fewest observations. There is a parameter `seasonality_prior_scale` which similarly adjusts the extent to which the seasonality model will fit the data.



================================================
FILE: docs/_docs/installation.md
================================================
---
layout: docs
docid: "installation"
title: "Installation"
permalink: /docs/installation.html
subsections:
  - id: r
    title: Using R
  - id: python
    title: Using Python
---

Prophet has two implementations: [R](#installation-in-r) and [Python](#installation-in-python).

<a href="#r"></a>

## Installation in R

Prophet is a [CRAN package](https://cran.r-project.org/package=prophet) so you can use `install.packages`.

```r
# R
install.packages('prophet')
```

After installation, you can [get started!](quick_start.html#r-api)

#### Experimental backend - cmdstanr

You can also choose an experimental alternative stan backend called `cmdstanr`. Once you've installed `prophet`,
follow these instructions to use `cmdstanr` instead of `rstan` as the backend:

```r
# R
# We recommend running this is a fresh R session or restarting your current session
install.packages(c("cmdstanr", "posterior"), repos = c("https://mc-stan.org/r-packages/", getOption("repos")))

# If you haven't installed cmdstan before, run:
cmdstanr::install_cmdstan()
# Otherwise, you can point cmdstanr to your cmdstan path:
cmdstanr::set_cmdstan_path(path = <your existing cmdstan>)

# Set the R_STAN_BACKEND environment variable
Sys.setenv(R_STAN_BACKEND = "CMDSTANR")
```

### Windows

On Windows, R requires a compiler so you'll need to [follow the instructions](https://github.com/stan-dev/rstan/wiki/Configuring-C---Toolchain-for-Windows) provided by `rstan`. The key step is installing [Rtools](http://cran.r-project.org/bin/windows/Rtools/) before attempting to install the package.

If you have custom Stan compiler settings, install from source rather than the CRAN binary.

<a href="#python"></a>

## Installation in Python

Prophet is on PyPI, so you can use `pip` to install it.

```bash
python -m pip install prophet
```

* From v0.6 onwards, Python 2 is no longer supported.
* As of v1.0, the package name on PyPI is "prophet"; prior to v1.0 it was "fbprophet".
* As of v1.1, the minimum supported Python version is 3.7.

After installation, you can [get started!](quick_start.html#python-api)

### Anaconda

Prophet can also be installed through conda-forge.

```bash
conda install -c conda-forge prophet
```



================================================
FILE: docs/_docs/multiplicative_seasonality.md
================================================
---
layout: docs
docid: "multiplicative_seasonality"
title: "Multiplicative Seasonality"
permalink: /docs/multiplicative_seasonality.html
subsections:
---
By default Prophet fits additive seasonalities, meaning the effect of the seasonality is added to the trend to get the forecast. This time series of the number of air passengers is an example of when additive seasonality does not work:


```R
# R
df <- read.csv('https://raw.githubusercontent.com/facebook/prophet/main/examples/example_air_passengers.csv')
m <- prophet(df)
future <- make_future_dataframe(m, 50, freq = 'm')
forecast <- predict(m, future)
plot(m, forecast)
```
```python
# Python
df = pd.read_csv('https://raw.githubusercontent.com/facebook/prophet/main/examples/example_air_passengers.csv')
m = Prophet()
m.fit(df)
future = m.make_future_dataframe(50, freq='MS')
forecast = m.predict(future)
fig = m.plot(forecast)
```

![png](/prophet/static/multiplicative_seasonality_files/multiplicative_seasonality_4_0.png)


This time series has a clear yearly cycle, but the seasonality in the forecast is too large at the start of the time series and too small at the end. In this time series, the seasonality is not a constant additive factor as assumed by Prophet, rather it grows with the trend. This is multiplicative seasonality.



Prophet can model multiplicative seasonality by setting `seasonality_mode='multiplicative'` in the input arguments:


```R
# R
m <- prophet(df, seasonality.mode = 'multiplicative')
forecast <- predict(m, future)
plot(m, forecast)
```
```python
# Python
m = Prophet(seasonality_mode='multiplicative')
m.fit(df)
forecast = m.predict(future)
fig = m.plot(forecast)
```

![png](/prophet/static/multiplicative_seasonality_files/multiplicative_seasonality_7_0.png)


The components figure will now show the seasonality as a percent of the trend:


```R
# R
prophet_plot_components(m, forecast)
```
```python
# Python
fig = m.plot_components(forecast)
```

![png](/prophet/static/multiplicative_seasonality_files/multiplicative_seasonality_10_0.png)


With `seasonality_mode='multiplicative'`, holiday effects will also be modeled as multiplicative. Any added seasonalities or extra regressors will by default use whatever `seasonality_mode` is set to, but can be overriden by specifying `mode='additive'` or `mode='multiplicative'` as an argument when adding the seasonality or regressor.



For example, this block sets the built-in seasonalities to multiplicative, but includes an additive quarterly seasonality and an additive regressor:


```R
# R
m <- prophet(seasonality.mode = 'multiplicative')
m <- add_seasonality(m, 'quarterly', period = 91.25, fourier.order = 8, mode = 'additive')
m <- add_regressor(m, 'regressor', mode = 'additive')
```
```python
# Python
m = Prophet(seasonality_mode='multiplicative')
m.add_seasonality('quarterly', period=91.25, fourier_order=8, mode='additive')
m.add_regressor('regressor', mode='additive')
```
Additive and multiplicative extra regressors will show up in separate panels on the components plot. Note, however, that it is pretty unlikely to have a mix of additive and multiplicative seasonalities, so this will generally only be used if there is a reason to expect that to be the case.




================================================
FILE: docs/_docs/non-daily_data.md
================================================
---
layout: docs
docid: "non-daily_data"
title: "Non-Daily Data"
permalink: /docs/non-daily_data.html
subsections:
  - title: Sub-daily data
    id: sub-daily-data
  - title: Data with regular gaps
    id: data-with-regular-gaps
  - title: Monthly data
    id: monthly-data
  - title: Holidays with aggregated data
    id: holidays-with-aggregated-data
---
<a id="sub-daily-data"> </a>

## Sub-daily data



Prophet can make forecasts for time series with sub-daily observations by passing in a dataframe with timestamps in the `ds` column. The format of the timestamps should be YYYY-MM-DD HH:MM:SS - see the example csv [here](https://github.com/facebook/prophet/blob/main/examples/example_yosemite_temps.csv). When sub-daily data are used, daily seasonality will automatically be fit. Here we fit Prophet to data with 5-minute resolution (daily temperatures at Yosemite):


```R
# R
df <- read.csv('https://raw.githubusercontent.com/facebook/prophet/main/examples/example_yosemite_temps.csv')
m <- prophet(df, changepoint.prior.scale=0.01)
future <- make_future_dataframe(m, periods = 300, freq = 60 * 60)
fcst <- predict(m, future)
plot(m, fcst)
```
```python
# Python
df = pd.read_csv('https://raw.githubusercontent.com/facebook/prophet/main/examples/example_yosemite_temps.csv')
m = Prophet(changepoint_prior_scale=0.01).fit(df)
future = m.make_future_dataframe(periods=300, freq='H')
fcst = m.predict(future)
fig = m.plot(fcst)
```

![png](/prophet/static/non-daily_data_files/non-daily_data_4_0.png)


The daily seasonality will show up in the components plot:


```R
# R
prophet_plot_components(m, fcst)
```
```python
# Python
fig = m.plot_components(fcst)
```

![png](/prophet/static/non-daily_data_files/non-daily_data_7_0.png)


<a id="data-with-regular-gaps"> </a>

## Data with regular gaps



Suppose the dataset above only had observations from 12a to 6a:


```R
# R
df2 <- df %>%
  mutate(ds = as.POSIXct(ds, tz="GMT")) %>%
  filter(as.numeric(format(ds, "%H")) < 6)
m <- prophet(df2)
future <- make_future_dataframe(m, periods = 300, freq = 60 * 60)
fcst <- predict(m, future)
plot(m, fcst)
```
```python
# Python
df2 = df.copy()
df2['ds'] = pd.to_datetime(df2['ds'])
df2 = df2[df2['ds'].dt.hour < 6]
m = Prophet().fit(df2)
future = m.make_future_dataframe(periods=300, freq='H')
fcst = m.predict(future)
fig = m.plot(fcst)
```

![png](/prophet/static/non-daily_data_files/non-daily_data_10_0.png)


The forecast seems quite poor, with much larger fluctuations in the future than were seen in the history. The issue here is that we have fit a daily cycle to a time series that only has data for part of the day (12a to 6a). The daily seasonality is thus unconstrained for the remainder of the day and is not estimated well. The solution is to only make predictions for the time windows for which there are historical data. Here, that means to limit the `future` dataframe to have times from 12a to 6a:


```R
# R
future2 <- future %>% 
  filter(as.numeric(format(ds, "%H")) < 6)
fcst <- predict(m, future2)
plot(m, fcst)
```
```python
# Python
future2 = future.copy()
future2 = future2[future2['ds'].dt.hour < 6]
fcst = m.predict(future2)
fig = m.plot(fcst)
```

![png](/prophet/static/non-daily_data_files/non-daily_data_13_0.png)


The same principle applies to other datasets with regular gaps in the data. For example, if the history contains only weekdays, then predictions should only be made for weekdays since the weekly seasonality will not be well estimated for the weekends.



<a id="monthly-data"> </a>

## Monthly data



You can use Prophet to fit monthly data. However, the underlying model is continuous-time, which means that you can get strange results if you fit the model to monthly data and then ask for daily forecasts. Here we forecast US retail sales volume for the next 10 years:


```R
# R
df <- read.csv('https://raw.githubusercontent.com/facebook/prophet/main/examples/example_retail_sales.csv')
m <- prophet(df, seasonality.mode = 'multiplicative')
future <- make_future_dataframe(m, periods = 3652)
fcst <- predict(m, future)
plot(m, fcst)
```
```python
# Python
df = pd.read_csv('https://raw.githubusercontent.com/facebook/prophet/main/examples/example_retail_sales.csv')
m = Prophet(seasonality_mode='multiplicative').fit(df)
future = m.make_future_dataframe(periods=3652)
fcst = m.predict(future)
fig = m.plot(fcst)
```

![png](/prophet/static/non-daily_data_files/non-daily_data_16_0.png)


This is the same issue from above where the dataset has regular gaps. When we fit the yearly seasonality, it only has data for the first of each month and the seasonality components for the remaining days are unidentifiable and overfit. This can be clearly seen by doing MCMC to see uncertainty in the seasonality:


```R
# R
m <- prophet(df, seasonality.mode = 'multiplicative', mcmc.samples = 300)
fcst <- predict(m, future)
prophet_plot_components(m, fcst)
```
```python
# Python
m = Prophet(seasonality_mode='multiplicative', mcmc_samples=300).fit(df, show_progress=False)
fcst = m.predict(future)
fig = m.plot_components(fcst)
```
    WARNING:pystan:481 of 600 iterations saturated the maximum tree depth of 10 (80.2 %)
    WARNING:pystan:Run again with max_treedepth larger than 10 to avoid saturation



![png](/prophet/static/non-daily_data_files/non-daily_data_19_1.png)


The seasonality has low uncertainty at the start of each month where there are data points, but has very high posterior variance in between. When fitting Prophet to monthly data, only make monthly forecasts, which can be done by passing the frequency into `make_future_dataframe`:


```R
# R
future <- make_future_dataframe(m, periods = 120, freq = 'month')
fcst <- predict(m, future)
plot(m, fcst)
```
```python
# Python
future = m.make_future_dataframe(periods=120, freq='MS')
fcst = m.predict(future)
fig = m.plot(fcst)
```

![png](/prophet/static/non-daily_data_files/non-daily_data_22_0.png)


In Python, the frequency can be anything from the pandas list of frequency strings here: https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#timeseries-offset-aliases . Note that `MS` used here is month-start, meaning the data point is placed on the start of each month.



In monthly data, yearly seasonality can also be modeled with binary extra regressors. In particular, the model can use 12 extra regressors like `is_jan`, `is_feb`, etc. where `is_jan` is 1 if the date is in Jan and 0 otherwise. This approach would avoid the within-month unidentifiability seen above. Be sure to use `yearly_seasonality=False` if monthly extra regressors are being added.


<a id="holidays-with-aggregated-data"> </a>

## Holidays with aggregated data



Holiday effects are applied to the particular date on which the holiday was specified. With data that has been aggregated to weekly or monthly frequency, holidays that don't fall on the particular date used in the data will be ignored: for example, a Monday holiday in a weekly time series where each data point is on a Sunday. To include holiday effects in the model, the holiday will need to be moved to the date in the history dataframe for which the effect is desired. Note that with weekly or monthly aggregated data, many holiday effects will be well-captured by the yearly seasonality, so added holidays may only be necessary for holidays that occur in different weeks throughout the time series.




================================================
FILE: docs/_docs/outliers.md
================================================
---
layout: docs
docid: "outliers"
title: "Outliers"
permalink: /docs/outliers.html
subsections:
---
There are two main ways that outliers can affect Prophet forecasts. Here we make a forecast on the logged Wikipedia visits to the R page from before, but with a block of bad data:


```R
# R
df <- read.csv('https://raw.githubusercontent.com/facebook/prophet/main/examples/example_wp_log_R_outliers1.csv')
m <- prophet(df)
future <- make_future_dataframe(m, periods = 1096)
forecast <- predict(m, future)
plot(m, forecast)
```
```python
# Python
df = pd.read_csv('https://raw.githubusercontent.com/facebook/prophet/main/examples/example_wp_log_R_outliers1.csv')
m = Prophet()
m.fit(df)
future = m.make_future_dataframe(periods=1096)
forecast = m.predict(future)
fig = m.plot(forecast)
```

![png](/prophet/static/outliers_files/outliers_4_0.png)


The trend forecast seems reasonable, but the uncertainty intervals seem way too wide. Prophet is able to handle the outliers in the history, but only by fitting them with trend changes. The uncertainty model then expects future trend changes of similar magnitude.



The best way to handle outliers is to remove them - Prophet has no problem with missing data. If you set their values to `NA` in the history but leave the dates in `future`, then Prophet will give you a prediction for their values.


```R
# R
outliers <- (as.Date(df$ds) > as.Date('2010-01-01')
             & as.Date(df$ds) < as.Date('2011-01-01'))
df$y[outliers] = NA
m <- prophet(df)
forecast <- predict(m, future)
plot(m, forecast)
```
```python
# Python
df.loc[(df['ds'] > '2010-01-01') & (df['ds'] < '2011-01-01'), 'y'] = None
model = Prophet().fit(df)
fig = model.plot(model.predict(future))
```

![png](/prophet/static/outliers_files/outliers_7_0.png)


In the above example the outliers messed up the uncertainty estimation but did not impact the main forecast `yhat`. This isn't always the case, as in this example with added outliers:


```R
# R
df <- read.csv('https://raw.githubusercontent.com/facebook/prophet/main/examples/example_wp_log_R_outliers2.csv')
m <- prophet(df)
future <- make_future_dataframe(m, periods = 1096)
forecast <- predict(m, future)
plot(m, forecast)
```
```python
# Python
df = pd.read_csv('https://raw.githubusercontent.com/facebook/prophet/main/examples/example_wp_log_R_outliers2.csv')
m = Prophet()
m.fit(df)
future = m.make_future_dataframe(periods=1096)
forecast = m.predict(future)
fig = m.plot(forecast)
```

![png](/prophet/static/outliers_files/outliers_10_0.png)


Here a group of extreme outliers in June 2015 mess up the seasonality estimate, so their effect reverberates into the future forever. Again the right approach is to remove them:


```R
# R
outliers <- (as.Date(df$ds) > as.Date('2015-06-01')
             & as.Date(df$ds) < as.Date('2015-06-30'))
df$y[outliers] = NA
m <- prophet(df)
forecast <- predict(m, future)
plot(m, forecast)
```
```python
# Python
df.loc[(df['ds'] > '2015-06-01') & (df['ds'] < '2015-06-30'), 'y'] = None
m = Prophet().fit(df)
fig = m.plot(m.predict(future))
```

![png](/prophet/static/outliers_files/outliers_13_0.png)




================================================
FILE: docs/_docs/quick_start.md
================================================
---
layout: docs
docid: "quick_start"
title: "Quick Start"
permalink: /docs/quick_start.html
subsections:
  - title: Python API
    id: python-api
  - title: R API
    id: r-api
---
<a id="python-api"> </a>

## Python API



Prophet follows the `sklearn` model API.  We create an instance of the `Prophet` class and then call its `fit` and `predict` methods.  


The input to Prophet is always a dataframe with two columns: `ds` and `y`.  The `ds` (datestamp) column should be of a format expected by Pandas, ideally YYYY-MM-DD for a date or YYYY-MM-DD HH:MM:SS for a timestamp. The `y` column must be numeric, and represents the measurement we wish to forecast.



As an example, let's look at a time series of the log daily page views for the Wikipedia page for [Peyton Manning](https://en.wikipedia.org/wiki/Peyton_Manning).  We scraped this data using the [Wikipediatrend](https://cran.r-project.org/package=wikipediatrend) package in R.  Peyton Manning provides a nice example because it illustrates some of Prophet's features, like multiple seasonality, changing growth rates, and the ability to model special days (such as Manning's playoff and superbowl appearances). The CSV is available [here](https://github.com/facebook/prophet/blob/main/examples/example_wp_log_peyton_manning.csv).



First we'll import the data:


```python
# Python
import pandas as pd
from prophet import Prophet

```
```python
# Python
df = pd.read_csv('https://raw.githubusercontent.com/facebook/prophet/main/examples/example_wp_log_peyton_manning.csv')
df.head()

```



<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>ds</th>
      <th>y</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>2007-12-10</td>
      <td>9.590761</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2007-12-11</td>
      <td>8.519590</td>
    </tr>
    <tr>
      <th>2</th>
      <td>2007-12-12</td>
      <td>8.183677</td>
    </tr>
    <tr>
      <th>3</th>
      <td>2007-12-13</td>
      <td>8.072467</td>
    </tr>
    <tr>
      <th>4</th>
      <td>2007-12-14</td>
      <td>7.893572</td>
    </tr>
  </tbody>
</table>
</div>



We fit the model by instantiating a new `Prophet` object.  Any settings to the forecasting procedure are passed into the constructor.  Then you call its `fit` method and pass in the historical dataframe. Fitting should take 1-5 seconds.


```python
# Python
m = Prophet()
m.fit(df)

```
Predictions are then made on a dataframe with a column `ds` containing the dates for which a prediction is to be made. You can get a suitable dataframe that extends into the future a specified number of days using the helper method `Prophet.make_future_dataframe`. By default it will also include the dates from the history, so we will see the model fit as well. 


```python
# Python
future = m.make_future_dataframe(periods=365)
future.tail()

```



<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>ds</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>3265</th>
      <td>2017-01-15</td>
    </tr>
    <tr>
      <th>3266</th>
      <td>2017-01-16</td>
    </tr>
    <tr>
      <th>3267</th>
      <td>2017-01-17</td>
    </tr>
    <tr>
      <th>3268</th>
      <td>2017-01-18</td>
    </tr>
    <tr>
      <th>3269</th>
      <td>2017-01-19</td>
    </tr>
  </tbody>
</table>
</div>



The `predict` method will assign each row in `future` a predicted value which it names `yhat`.  If you pass in historical dates, it will provide an in-sample fit. The `forecast` object here is a new dataframe that includes a column `yhat` with the forecast, as well as columns for components and uncertainty intervals.


```python
# Python
forecast = m.predict(future)
forecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].tail()

```



<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>ds</th>
      <th>yhat</th>
      <th>yhat_lower</th>
      <th>yhat_upper</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>3265</th>
      <td>2017-01-15</td>
      <td>8.212625</td>
      <td>7.456310</td>
      <td>8.959726</td>
    </tr>
    <tr>
      <th>3266</th>
      <td>2017-01-16</td>
      <td>8.537635</td>
      <td>7.842986</td>
      <td>9.290934</td>
    </tr>
    <tr>
      <th>3267</th>
      <td>2017-01-17</td>
      <td>8.325071</td>
      <td>7.600879</td>
      <td>9.072006</td>
    </tr>
    <tr>
      <th>3268</th>
      <td>2017-01-18</td>
      <td>8.157723</td>
      <td>7.512052</td>
      <td>8.924022</td>
    </tr>
    <tr>
      <th>3269</th>
      <td>2017-01-19</td>
      <td>8.169677</td>
      <td>7.412473</td>
      <td>8.946977</td>
    </tr>
  </tbody>
</table>
</div>



You can plot the forecast by calling the `Prophet.plot` method and passing in your forecast dataframe.


```python
# Python
fig1 = m.plot(forecast)

```

![png](/prophet/static/quick_start_files/quick_start_12_0.png)


If you want to see the forecast components, you can use the `Prophet.plot_components` method.  By default you'll see the trend, yearly seasonality, and weekly seasonality of the time series.  If you include holidays, you'll see those here, too.


```python
# Python
fig2 = m.plot_components(forecast)

```

![png](/prophet/static/quick_start_files/quick_start_14_0.png)


An interactive figure of the forecast and components can be created with plotly. You will need to install plotly 4.0 or above separately, as it will not by default be installed with prophet. You will also need to install the `notebook` and `ipywidgets` packages.


```python
# Python
from prophet.plot import plot_plotly, plot_components_plotly

plot_plotly(m, forecast)

```
```python
# Python
plot_components_plotly(m, forecast)

```
More details about the options available for each method are available in the docstrings, for example, via `help(Prophet)` or `help(Prophet.fit)`.


<a id="r-api"> </a>

## R API



In R, we use the normal model fitting API.  We provide a `prophet` function that performs fitting and returns a model object.  You can then call `predict` and `plot` on this model object.


```R
# R
library(prophet)

```
    R[write to console]: Loading required package: Rcpp
    
    R[write to console]: Loading required package: rlang
    


First we read in the data and create the outcome variable. As in the Python API, this is a dataframe with columns `ds` and `y`, containing the date and numeric value respectively. The ds column should be YYYY-MM-DD for a date, or YYYY-MM-DD HH:MM:SS for a timestamp. As above, we use here the log number of views to Peyton Manning's Wikipedia page, available [here](https://github.com/facebook/prophet/blob/main/examples/example_wp_log_peyton_manning.csv).


```R
# R
df <- read.csv('https://raw.githubusercontent.com/facebook/prophet/main/examples/example_wp_log_peyton_manning.csv')

```
We call the `prophet` function to fit the model.  The first argument is the historical dataframe.  Additional arguments control how Prophet fits the data and are described in later pages of this documentation.


```R
# R
m <- prophet(df)

```
Predictions are made on a dataframe with a column `ds` containing the dates for which predictions are to be made. The `make_future_dataframe` function takes the model object and a number of periods to forecast and produces a suitable dataframe. By default it will also include the historical dates so we can evaluate in-sample fit.


```R
# R
future <- make_future_dataframe(m, periods = 365)
tail(future)

```
                 ds
    3265 2017-01-14
    3266 2017-01-15
    3267 2017-01-16
    3268 2017-01-17
    3269 2017-01-18
    3270 2017-01-19


As with most modeling procedures in R, we use the generic `predict` function to get our forecast. The `forecast` object is a dataframe with a column `yhat` containing the forecast. It has additional columns for uncertainty intervals and seasonal components.


```R
# R
forecast <- predict(m, future)
tail(forecast[c('ds', 'yhat', 'yhat_lower', 'yhat_upper')])

```
                 ds     yhat yhat_lower yhat_upper
    3265 2017-01-14 7.818359   7.071228   8.550957
    3266 2017-01-15 8.200125   7.475725   8.869495
    3267 2017-01-16 8.525104   7.747071   9.226915
    3268 2017-01-17 8.312482   7.551904   9.046774
    3269 2017-01-18 8.145098   7.390770   8.863692
    3270 2017-01-19 8.156964   7.381716   8.866507


You can use the generic `plot` function to plot the forecast, by passing in the model and the forecast dataframe.


```R
# R
plot(m, forecast)

```

![png](/prophet/static/quick_start_files/quick_start_30_0.png)


You can use the `prophet_plot_components` function to see the forecast broken down into trend, weekly seasonality, and yearly seasonality.


```R
# R
prophet_plot_components(m, forecast)

```

![png](/prophet/static/quick_start_files/quick_start_32_0.png)


An interactive plot of the forecast using Dygraphs can be made with the command `dyplot.prophet(m, forecast)`.



More details about the options available for each method are available in the docstrings, for example, via `?prophet` or `?fit.prophet`. This documentation is also available in the [reference manual](https://cran.r-project.org/web/packages/prophet/prophet.pdf) on CRAN.




================================================
FILE: docs/_docs/saturating_forecasts.md
================================================
---
layout: docs
docid: "saturating_forecasts"
title: "Saturating Forecasts"
permalink: /docs/saturating_forecasts.html
subsections:
  - title: Forecasting Growth
    id: forecasting-growth
  - title: Saturating Minimum
    id: saturating-minimum
---
<a id="forecasting-growth"> </a>

### Forecasting Growth



By default, Prophet uses a linear model for its forecast. When forecasting growth, there is usually some maximum achievable point: total market size, total population size, etc. This is called the carrying capacity, and the forecast should saturate at this point.



Prophet allows you to make forecasts using a [logistic growth](https://en.wikipedia.org/wiki/Logistic_function) trend model, with a specified carrying capacity. We illustrate this with the log number of page visits to the [R (programming language)](https://en.wikipedia.org/wiki/R_%28programming_language%29) page on Wikipedia:


```R
# R
df <- read.csv('https://raw.githubusercontent.com/facebook/prophet/main/examples/example_wp_log_peyton_manning.csv')
```
```python
# Python
df = pd.read_csv('https://raw.githubusercontent.com/facebook/prophet/main/examples/example_wp_log_peyton_manning.csv')
```
We must specify the carrying capacity in a column `cap`. Here we will assume a particular value, but this would usually be set using data or expertise about the market size.


```R
# R
df$cap <- 8.5
```
```python
# Python
df['cap'] = 8.5
```
The important things to note are that `cap` must be specified for every row in the dataframe, and that it does not have to be constant. If the market size is growing, then `cap` can be an increasing sequence.



We then fit the model as before, except pass in an additional argument to specify logistic growth:


```R
# R
m <- prophet(df, growth = 'logistic')
```
```python
# Python
m = Prophet(growth='logistic')
m.fit(df)
```
We make a dataframe for future predictions as before, except we must also specify the capacity in the future. Here we keep capacity constant at the same value as in the history, and forecast 5 years into the future:


```R
# R
future <- make_future_dataframe(m, periods = 1826)
future$cap <- 8.5
fcst <- predict(m, future)
plot(m, fcst)
```
```python
# Python
future = m.make_future_dataframe(periods=1826)
future['cap'] = 8.5
fcst = m.predict(future)
fig = m.plot(fcst)
```

![png](/prophet/static/saturating_forecasts_files/saturating_forecasts_13_0.png)


The logistic function has an implicit minimum of 0, and will saturate at 0 the same way that it saturates at the capacity. It is possible to also specify a different saturating minimum.



<a id="saturating-minimum"> </a>

### Saturating Minimum



The logistic growth model can also handle a saturating minimum, which is specified with a column `floor` in the same way as the `cap` column specifies the maximum:


```R
# R
df$y <- 10 - df$y
df$cap <- 6
df$floor <- 1.5
future$cap <- 6
future$floor <- 1.5
m <- prophet(df, growth = 'logistic')
fcst <- predict(m, future)
plot(m, fcst)
```
```python
# Python
df['y'] = 10 - df['y']
df['cap'] = 6
df['floor'] = 1.5
future['cap'] = 6
future['floor'] = 1.5
m = Prophet(growth='logistic')
m.fit(df)
fcst = m.predict(future)
fig = m.plot(fcst)
```

![png](/prophet/static/saturating_forecasts_files/saturating_forecasts_16_0.png)


To use a logistic growth trend with a saturating minimum, a maximum capacity must also be specified.




================================================
FILE: docs/_docs/seasonality,_holiday_effects,_and_regressors.md
================================================
---
layout: docs
docid: "seasonality,_holiday_effects,_and_regressors"
title: "Seasonality, Holiday Effects, And Regressors"
permalink: /docs/seasonality,_holiday_effects,_and_regressors.html
subsections:
  - title: Modeling Holidays and Special Events
    id: modeling-holidays-and-special-events
  - title: Built-in Country Holidays
    id: built-in-country-holidays
  - title: Holidays for subdivisions (Python)
    id: holidays-for-subdivisions-(python)
  - title: Fourier Order for Seasonalities
    id: fourier-order-for-seasonalities
  - title: Specifying Custom Seasonalities
    id: specifying-custom-seasonalities
  - title: Seasonalities that depend on other factors
    id: seasonalities-that-depend-on-other-factors
  - title: Prior scale for holidays and seasonality
    id: prior-scale-for-holidays-and-seasonality
  - title: Additional regressors
    id: additional-regressors
  - title: Coefficients of additional regressors
    id: coefficients-of-additional-regressors
---
<a id="modeling-holidays-and-special-events"> </a>

### Modeling Holidays and Special Events

If you have holidays or other recurring events that you'd like to model, you must create a dataframe for them. It has two columns (`holiday` and `ds`) and a row for each occurrence of the holiday. It must include all occurrences of the holiday, both in the past (back as far as the historical data go) and in the future (out as far as the forecast is being made). If they won't repeat in the future, Prophet will model them and then not include them in the forecast.



You can also include columns `lower_window` and `upper_window` which extend the holiday out to `[lower_window, upper_window]` days around the date. For instance, if you wanted to include Christmas Eve in addition to Christmas you'd include `lower_window=-1,upper_window=0`. If you wanted to use Black Friday in addition to Thanksgiving, you'd include `lower_window=0,upper_window=1`. You can also include a column `prior_scale` to set the prior scale separately for each holiday, as described below.



Here we create a dataframe that includes the dates of all of Peyton Manning's playoff appearances:


```R
# R
library(dplyr)
playoffs <- data_frame(
  holiday = 'playoff',
  ds = as.Date(c('2008-01-13', '2009-01-03', '2010-01-16',
                 '2010-01-24', '2010-02-07', '2011-01-08',
                 '2013-01-12', '2014-01-12', '2014-01-19',
                 '2014-02-02', '2015-01-11', '2016-01-17',
                 '2016-01-24', '2016-02-07')),
  lower_window = 0,
  upper_window = 1
)
superbowls <- data_frame(
  holiday = 'superbowl',
  ds = as.Date(c('2010-02-07', '2014-02-02', '2016-02-07')),
  lower_window = 0,
  upper_window = 1
)
holidays <- bind_rows(playoffs, superbowls)
```
```python
# Python
playoffs = pd.DataFrame({
  'holiday': 'playoff',
  'ds': pd.to_datetime(['2008-01-13', '2009-01-03', '2010-01-16',
                        '2010-01-24', '2010-02-07', '2011-01-08',
                        '2013-01-12', '2014-01-12', '2014-01-19',
                        '2014-02-02', '2015-01-11', '2016-01-17',
                        '2016-01-24', '2016-02-07']),
  'lower_window': 0,
  'upper_window': 1,
})
superbowls = pd.DataFrame({
  'holiday': 'superbowl',
  'ds': pd.to_datetime(['2010-02-07', '2014-02-02', '2016-02-07']),
  'lower_window': 0,
  'upper_window': 1,
})
holidays = pd.concat((playoffs, superbowls))
```
Above we have included the superbowl days as both playoff games and superbowl games. This means that the superbowl effect will be an additional additive bonus on top of the playoff effect.



Once the table is created, holiday effects are included in the forecast by passing them in with the `holidays` argument. Here we do it with the Peyton Manning data from the [Quickstart](https://facebook.github.io/prophet/docs/quick_start.html):


```R
# R
m <- prophet(df, holidays = holidays)
forecast <- predict(m, future)
```
```python
# Python
m = Prophet(holidays=holidays)
forecast = m.fit(df).predict(future)
```
The holiday effect can be seen in the `forecast` dataframe:


```R
# R
forecast %>% 
  select(ds, playoff, superbowl) %>% 
  filter(abs(playoff + superbowl) > 0) %>%
  tail(10)
```
```python
# Python
forecast[(forecast['playoff'] + forecast['superbowl']).abs() > 0][
        ['ds', 'playoff', 'superbowl']][-10:]
```



<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>ds</th>
      <th>playoff</th>
      <th>superbowl</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>2190</th>
      <td>2014-02-02</td>
      <td>1.223965</td>
      <td>1.201517</td>
    </tr>
    <tr>
      <th>2191</th>
      <td>2014-02-03</td>
      <td>1.901742</td>
      <td>1.460471</td>
    </tr>
    <tr>
      <th>2532</th>
      <td>2015-01-11</td>
      <td>1.223965</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>2533</th>
      <td>2015-01-12</td>
      <td>1.901742</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>2901</th>
      <td>2016-01-17</td>
      <td>1.223965</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>2902</th>
      <td>2016-01-18</td>
      <td>1.901742</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>2908</th>
      <td>2016-01-24</td>
      <td>1.223965</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>2909</th>
      <td>2016-01-25</td>
      <td>1.901742</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>2922</th>
      <td>2016-02-07</td>
      <td>1.223965</td>
      <td>1.201517</td>
    </tr>
    <tr>
      <th>2923</th>
      <td>2016-02-08</td>
      <td>1.901742</td>
      <td>1.460471</td>
    </tr>
  </tbody>
</table>
</div>



The holiday effects will also show up in the components plot, where we see that there is a spike on the days around playoff appearances, with an especially large spike for the superbowl:


```R
# R
prophet_plot_components(m, forecast)
```
```python
# Python
fig = m.plot_components(forecast)
```

![png](/prophet/static/seasonality,_holiday_effects,_and_regressors_files/seasonality,_holiday_effects,_and_regressors_14_0.png)


Individual holidays can be plotted using the `plot_forecast_component` function (imported from `prophet.plot` in Python) like `plot_forecast_component(m, forecast, 'superbowl')` to plot just the superbowl holiday component.


<a id="built-in-country-holidays"> </a>

### Built-in Country Holidays



You can use a built-in collection of country-specific holidays using the `add_country_holidays` method (Python) or function (R). The name of the country is specified, and then major holidays for that country will be included in addition to any holidays that are specified via the `holidays` argument described above:


```R
# R
m <- prophet(holidays = holidays)
m <- add_country_holidays(m, country_name = 'US')
m <- fit.prophet(m, df)
```
```python
# Python
m = Prophet(holidays=holidays)
m.add_country_holidays(country_name='US')
m.fit(df)
```
You can see which holidays were included by looking at the `train_holiday_names` (Python) or `train.holiday.names` (R) attribute of the model:


```R
# R
m$train.holiday.names
```
     [1] "playoff"                     "superbowl"                  
     [3] "New Year's Day"              "Martin Luther King Jr. Day" 
     [5] "Washington's Birthday"       "Memorial Day"               
     [7] "Independence Day"            "Labor Day"                  
     [9] "Columbus Day"                "Veterans Day"               
    [11] "Veterans Day (Observed)"     "Thanksgiving"               
    [13] "Christmas Day"               "Independence Day (Observed)"
    [15] "Christmas Day (Observed)"    "New Year's Day (Observed)"  


```python
# Python
m.train_holiday_names
```



    0                         playoff
    1                       superbowl
    2                  New Year's Day
    3      Martin Luther King Jr. Day
    4           Washington's Birthday
    5                    Memorial Day
    6                Independence Day
    7                       Labor Day
    8                    Columbus Day
    9                    Veterans Day
    10                   Thanksgiving
    11                  Christmas Day
    12       Christmas Day (Observed)
    13        Veterans Day (Observed)
    14    Independence Day (Observed)
    15      New Year's Day (Observed)
    dtype: object



The holidays for each country are provided by the `holidays` package in Python. A list of available countries, and the country name to use, is available on their page: https://github.com/vacanza/python-holidays/.



In Python, most holidays are computed deterministically and so are available for any date range; a warning will be raised if dates fall outside the range supported by that country. In R, holiday dates are computed for 1995 through 2044 and stored in the package as `data-raw/generated_holidays.csv`. If a wider date range is needed, this script can be used to replace that file with a different date range: https://github.com/facebook/prophet/blob/main/python/scripts/generate_holidays_file.py.



As above, the country-level holidays will then show up in the components plot:


```R
# R
forecast <- predict(m, future)
prophet_plot_components(m, forecast)
```
```python
# Python
forecast = m.predict(future)
fig = m.plot_components(forecast)
```

![png](/prophet/static/seasonality,_holiday_effects,_and_regressors_files/seasonality,_holiday_effects,_and_regressors_24_0.png)


<a id="holidays-for-subdivisions-(python)"> </a>

#### Holidays for subdivisions (Python)



We can use the utility function `make_holidays_df` to easily create custom holidays DataFrames, e.g. for certain states, using data from the [holidays](https://pypi.org/project/holidays/) package. This can be passed directly to the `Prophet()` constructor.


```python
# Python
from prophet.make_holidays import make_holidays_df

nsw_holidays = make_holidays_df(
    year_list=[2019 + i for i in range(10)], country='AU', province='NSW'
)
nsw_holidays.head(n=10)
```



<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>ds</th>
      <th>holiday</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>2019-01-26</td>
      <td>Australia Day</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2019-01-28</td>
      <td>Australia Day (Observed)</td>
    </tr>
    <tr>
      <th>2</th>
      <td>2019-04-25</td>
      <td>Anzac Day</td>
    </tr>
    <tr>
      <th>3</th>
      <td>2019-12-25</td>
      <td>Christmas Day</td>
    </tr>
    <tr>
      <th>4</th>
      <td>2019-12-26</td>
      <td>Boxing Day</td>
    </tr>
    <tr>
      <th>5</th>
      <td>2019-04-20</td>
      <td>Easter Saturday</td>
    </tr>
    <tr>
      <th>6</th>
      <td>2019-04-21</td>
      <td>Easter Sunday</td>
    </tr>
    <tr>
      <th>7</th>
      <td>2019-10-07</td>
      <td>Labour Day</td>
    </tr>
    <tr>
      <th>8</th>
      <td>2019-06-10</td>
      <td>Queen's Birthday</td>
    </tr>
    <tr>
      <th>9</th>
      <td>2019-08-05</td>
      <td>Bank Holiday</td>
    </tr>
  </tbody>
</table>
</div>



```python
# Python
from prophet import Prophet

m_nsw = Prophet(holidays=nsw_holidays)
```
<a id="fourier-order-for-seasonalities"> </a>

### Fourier Order for Seasonalities



Seasonalities are estimated using a partial Fourier sum. See [the paper](https://peerj.com/preprints/3190/) for complete details, and [this figure on Wikipedia](https://en.wikipedia.org/wiki/Fourier_series#/media/File:Fourier_Series.svg) for an illustration of how a partial Fourier sum can approximate an arbitrary periodic signal. The number of terms in the partial sum (the order) is a parameter that determines how quickly the seasonality can change. To illustrate this, consider the Peyton Manning data from the [Quickstart](https://facebook.github.io/prophet/docs/quick_start.html). The default Fourier order for yearly seasonality is 10, which produces this fit:


```R
# R
m <- prophet(df)
prophet:::plot_yearly(m)
```
```python
# Python
from prophet.plot import plot_yearly
m = Prophet().fit(df)
a = plot_yearly(m)
```

![png](/prophet/static/seasonality,_holiday_effects,_and_regressors_files/seasonality,_holiday_effects,_and_regressors_30_0.png)


The default values are often appropriate, but they can be increased when the seasonality needs to fit higher-frequency changes, and generally be less smooth. The Fourier order can be specified for each built-in seasonality when instantiating the model, here it is increased to 20:


```R
# R
m <- prophet(df, yearly.seasonality = 20)
prophet:::plot_yearly(m)
```
```python
# Python
from prophet.plot import plot_yearly
m = Prophet(yearly_seasonality=20).fit(df)
a = plot_yearly(m)
```

![png](/prophet/static/seasonality,_holiday_effects,_and_regressors_files/seasonality,_holiday_effects,_and_regressors_33_0.png)


Increasing the number of Fourier terms allows the seasonality to fit faster changing cycles, but can also lead to overfitting: N Fourier terms corresponds to 2N variables used for modeling the cycle



<a id="specifying-custom-seasonalities"> </a>

### Specifying Custom Seasonalities



Prophet will by default fit weekly and yearly seasonalities, if the time series is more than two cycles long. It will also fit daily seasonality for a sub-daily time series. You can add other seasonalities (monthly, quarterly, hourly) using the `add_seasonality` method (Python) or function (R).



The inputs to this function are a name, the period of the seasonality in days, and the Fourier order for the seasonality. For reference, by default Prophet uses a Fourier order of 3 for weekly seasonality and 10 for yearly seasonality. An optional input to `add_seasonality` is the prior scale for that seasonal component - this is discussed below.



As an example, here we fit the Peyton Manning data from the [Quickstart](https://facebook.github.io/prophet/docs/quick_start.html), but replace the weekly seasonality with monthly seasonality. The monthly seasonality then will appear in the components plot:


```R
# R
m <- prophet(weekly.seasonality=FALSE)
m <- add_seasonality(m, name='monthly', period=30.5, fourier.order=5)
m <- fit.prophet(m, df)
forecast <- predict(m, future)
prophet_plot_components(m, forecast)
```
```python
# Python
m = Prophet(weekly_seasonality=False)
m.add_seasonality(name='monthly', period=30.5, fourier_order=5)
forecast = m.fit(df).predict(future)
fig = m.plot_components(forecast)
```

![png](/prophet/static/seasonality,_holiday_effects,_and_regressors_files/seasonality,_holiday_effects,_and_regressors_36_0.png)


<a id="seasonalities-that-depend-on-other-factors"> </a>

### Seasonalities that depend on other factors

In some instances the seasonality may depend on other factors, such as a weekly seasonal pattern that is different during the summer than it is during the rest of the year, or a daily seasonal pattern that is different on weekends vs. on weekdays. These types of seasonalities can be modeled using conditional seasonalities.



Consider the Peyton Manning example from the [Quickstart](https://facebook.github.io/prophet/docs/quick_start.html). The default weekly seasonality assumes that the pattern of weekly seasonality is the same throughout the year, but we'd expect the pattern of weekly seasonality to be different during the on-season (when there are games every Sunday) and the off-season. We can use conditional seasonalities to construct separate on-season and off-season weekly seasonalities.



First we add a boolean column to the dataframe that indicates whether each date is during the on-season or the off-season:


```R
# R
is_nfl_season <- function(ds) {
  dates <- as.Date(ds)
  month <- as.numeric(format(dates, '%m'))
  return(month > 8 | month < 2)
}
df$on_season <- is_nfl_season(df$ds)
df$off_season <- !is_nfl_season(df$ds)
```
```python
# Python
def is_nfl_season(ds):
    date = pd.to_datetime(ds)
    return (date.month > 8 or date.month < 2)

df['on_season'] = df['ds'].apply(is_nfl_season)
df['off_season'] = ~df['ds'].apply(is_nfl_season)
```
Then we disable the built-in weekly seasonality, and replace it with two weekly seasonalities that have these columns specified as a condition. This means that the seasonality will only be applied to dates where the `condition_name` column is `True`. We must also add the column to the `future` dataframe for which we are making predictions.


```R
# R
m <- prophet(weekly.seasonality=FALSE)
m <- add_seasonality(m, name='weekly_on_season', period=7, fourier.order=3, condition.name='on_season')
m <- add_seasonality(m, name='weekly_off_season', period=7, fourier.order=3, condition.name='off_season')
m <- fit.prophet(m, df)

future$on_season <- is_nfl_season(future$ds)
future$off_season <- !is_nfl_season(future$ds)
forecast <- predict(m, future)
prophet_plot_components(m, forecast)
```
```python
# Python
m = Prophet(weekly_seasonality=False)
m.add_seasonality(name='weekly_on_season', period=7, fourier_order=3, condition_name='on_season')
m.add_seasonality(name='weekly_off_season', period=7, fourier_order=3, condition_name='off_season')

future['on_season'] = future['ds'].apply(is_nfl_season)
future['off_season'] = ~future['ds'].apply(is_nfl_season)
forecast = m.fit(df).predict(future)
fig = m.plot_components(forecast)
```

![png](/prophet/static/seasonality,_holiday_effects,_and_regressors_files/seasonality,_holiday_effects,_and_regressors_42_0.png)


Both of the seasonalities now show up in the components plots above. We can see that during the on-season when games are played every Sunday, there are large increases on Sunday and Monday that are completely absent during the off-season.


<a id="prior-scale-for-holidays-and-seasonality"> </a>

### Prior scale for holidays and seasonality

If you find that the holidays are overfitting, you can adjust their prior scale to smooth them using the parameter `holidays_prior_scale`. By default this parameter is 10, which provides very little regularization. Reducing this parameter dampens holiday effects:


```R
# R
m <- prophet(df, holidays = holidays, holidays.prior.scale = 0.05)
forecast <- predict(m, future)
forecast %>% 
  select(ds, playoff, superbowl) %>% 
  filter(abs(playoff + superbowl) > 0) %>%
  tail(10)
```
```python
# Python
m = Prophet(holidays=holidays, holidays_prior_scale=0.05).fit(df)
forecast = m.predict(future)
forecast[(forecast['playoff'] + forecast['superbowl']).abs() > 0][
    ['ds', 'playoff', 'superbowl']][-10:]
```



<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>ds</th>
      <th>playoff</th>
      <th>superbowl</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>2190</th>
      <td>2014-02-02</td>
      <td>1.206086</td>
      <td>0.964914</td>
    </tr>
    <tr>
      <th>2191</th>
      <td>2014-02-03</td>
      <td>1.852077</td>
      <td>0.992634</td>
    </tr>
    <tr>
      <th>2532</th>
      <td>2015-01-11</td>
      <td>1.206086</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>2533</th>
      <td>2015-01-12</td>
      <td>1.852077</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>2901</th>
      <td>2016-01-17</td>
      <td>1.206086</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>2902</th>
      <td>2016-01-18</td>
      <td>1.852077</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>2908</th>
      <td>2016-01-24</td>
      <td>1.206086</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>2909</th>
      <td>2016-01-25</td>
      <td>1.852077</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>2922</th>
      <td>2016-02-07</td>
      <td>1.206086</td>
      <td>0.964914</td>
    </tr>
    <tr>
      <th>2923</th>
      <td>2016-02-08</td>
      <td>1.852077</td>
      <td>0.992634</td>
    </tr>
  </tbody>
</table>
</div>



The magnitude of the holiday effect has been reduced compared to before, especially for superbowls, which had the fewest observations. There is a parameter `seasonality_prior_scale` which similarly adjusts the extent to which the seasonality model will fit the data.



Prior scales can be set separately for individual holidays by including a column `prior_scale` in the holidays dataframe. Prior scales for individual seasonalities can be passed as an argument to `add_seasonality`. For instance, the prior scale for just weekly seasonality can be set using:


```R
# R
m <- prophet()
m <- add_seasonality(
  m, name='weekly', period=7, fourier.order=3, prior.scale=0.1)
```
```python
# Python
m = Prophet()
m.add_seasonality(
    name='weekly', period=7, fourier_order=3, prior_scale=0.1)
```


<a id="additional-regressors"> </a>

### Additional regressors

Additional regressors can be added to the linear part of the model using the `add_regressor` method or function. A column with the regressor value will need to be present in both the fitting and prediction dataframes. For example, we can add an additional effect on Sundays during the NFL season. On the components plot, this effect will show up in the 'extra_regressors' plot:


```R
# R
nfl_sunday <- function(ds) {
  dates <- as.Date(ds)
  month <- as.numeric(format(dates, '%m'))
  as.numeric((weekdays(dates) == "Sunday") & (month > 8 | month < 2))
}
df$nfl_sunday <- nfl_sunday(df$ds)

m <- prophet()
m <- add_regressor(m, 'nfl_sunday')
m <- fit.prophet(m, df)

future$nfl_sunday <- nfl_sunday(future$ds)

forecast <- predict(m, future)
prophet_plot_components(m, forecast)
```
```python
# Python
def nfl_sunday(ds):
    date = pd.to_datetime(ds)
    if date.weekday() == 6 and (date.month > 8 or date.month < 2):
        return 1
    else:
        return 0
df['nfl_sunday'] = df['ds'].apply(nfl_sunday)

m = Prophet()
m.add_regressor('nfl_sunday')
m.fit(df)

future['nfl_sunday'] = future['ds'].apply(nfl_sunday)

forecast = m.predict(future)
fig = m.plot_components(forecast)
```

![png](/prophet/static/seasonality,_holiday_effects,_and_regressors_files/seasonality,_holiday_effects,_and_regressors_52_0.png)


NFL Sundays could also have been handled using the "holidays" interface described above, by creating a list of past and future NFL Sundays. The `add_regressor` function provides a more general interface for defining extra linear regressors, and in particular does not require that the regressor be a binary indicator. Another time series could be used as a regressor, although its future values would have to be known.



[This notebook](https://nbviewer.jupyter.org/github/nicolasfauchereau/Auckland_Cycling/blob/master/notebooks/Auckland_cycling_and_weather.ipynb) shows an example of using weather factors as extra regressors in a forecast of bicycle usage, and provides an excellent illustration of how other time series can be included as extra regressors.



The `add_regressor` function has optional arguments for specifying the prior scale (holiday prior scale is used by default) and whether or not the regressor is standardized - see the docstring with `help(Prophet.add_regressor)` in Python and `?add_regressor` in R. Note that regressors must be added prior to model fitting. Prophet will also raise an error if the regressor is constant throughout the history, since there is nothing to fit from it.



The extra regressor must be known for both the history and for future dates. It thus must either be something that has known future values (such as `nfl_sunday`), or something that has separately been forecasted elsewhere. The weather regressors used in the notebook linked above is a good example of an extra regressor that has forecasts that can be used for future values. 



One can also use as a regressor another time series that has been forecasted with a time series model, such as Prophet. For instance, if `r(t)` is included as a regressor for `y(t)`, Prophet can be used to forecast `r(t)` and then that forecast can be plugged in as the future values when forecasting `y(t)`. A note of caution around this approach: This will probably not be useful unless `r(t)` is somehow easier to forecast than `y(t)`. This is because error in the forecast of `r(t)` will produce error in the forecast of `y(t)`. One setting where this can be useful is in hierarchical time series, where there is top-level forecast that has higher signal-to-noise and is thus easier to forecast. Its forecast can be included in the forecast for each lower-level series.



Extra regressors are put in the linear component of the model, so the underlying model is that the time series depends on the extra regressor as either an additive or multiplicative factor (see the next section for multiplicativity).



<a id="coefficients-of-additional-regressors"> </a>

#### Coefficients of additional regressors



To extract the beta coefficients of the extra regressors, use the utility function `regressor_coefficients` (`from prophet.utilities import regressor_coefficients` in Python, `prophet::regressor_coefficients` in R) on the fitted model. The estimated beta coefficient for each regressor roughly represents the increase in prediction value for a unit increase in the regressor value (note that the coefficients returned are always on the scale of the original data). If `mcmc_samples` is specified, a credible interval for each coefficient is also returned, which can help identify whether the regressor is meaningful to the model (a credible interval that includes the 0 value suggests the regressor is not meaningful).




================================================
FILE: docs/_docs/trend_changepoints.md
================================================
---
layout: docs
docid: "trend_changepoints"
title: "Trend Changepoints"
permalink: /docs/trend_changepoints.html
subsections:
  - title: Automatic changepoint detection in Prophet
    id: automatic-changepoint-detection-in-prophet
  - title: Adjusting trend flexibility
    id: adjusting-trend-flexibility
  - title: Specifying the locations of the changepoints
    id: specifying-the-locations-of-the-changepoints
---
You may have noticed in the earlier examples in this documentation that real time series frequently have abrupt changes in their trajectories. By default, Prophet will automatically detect these changepoints and will allow the trend to adapt appropriately. However, if you wish to have finer control over this process (e.g., Prophet missed a rate change, or is overfitting rate changes in the history), then there are several input arguments you can use.


<a id="automatic-changepoint-detection-in-prophet"> </a>

### Automatic changepoint detection in Prophet

Prophet detects changepoints by first specifying a large number of *potential changepoints* at which the rate is allowed to change. It then puts a sparse prior on the magnitudes of the rate changes (equivalent to L1 regularization) - this essentially means that Prophet has a large number of *possible* places where the rate can change, but will use as few of them as possible. Consider the Peyton Manning forecast from the Quickstart. By default, Prophet specifies 25 potential changepoints which are uniformly placed in the first 80% of the time series. The vertical lines in this figure indicate where the potential changepoints were placed:



![png](/prophet/static/trend_changepoints_files/trend_changepoints_4_0.png)


Even though we have a lot of places where the rate can possibly change, because of the sparse prior, most of these changepoints go unused. We can see this by plotting the magnitude of the rate change at each changepoint:



![png](/prophet/static/trend_changepoints_files/trend_changepoints_6_0.png)


The number of potential changepoints can be set using the argument `n_changepoints`, but this is better tuned by adjusting the regularization. The locations of the signification changepoints can be visualized with:


```R
# R
plot(m, forecast) + add_changepoints_to_plot(m)
```
```python
# Python
from prophet.plot import add_changepoints_to_plot
fig = m.plot(forecast)
a = add_changepoints_to_plot(fig.gca(), m, forecast)
```

![png](/prophet/static/trend_changepoints_files/trend_changepoints_9_0.png)


By default changepoints are only inferred for the first 80% of the time series in order to have plenty of runway for projecting the trend forward and to avoid overfitting fluctuations at the end of the time series. This default works in many situations but not all, and can be changed using the `changepoint_range` argument. For example, `m = Prophet(changepoint_range=0.9)` in Python or `m <- prophet(changepoint.range = 0.9)` in R will place potential changepoints in the first 90% of the time series.


<a id="adjusting-trend-flexibility"> </a>

### Adjusting trend flexibility

If the trend changes are being overfit (too much flexibility) or underfit (not enough flexibility), you can adjust the strength of the sparse prior using the input argument `changepoint_prior_scale`. By default, this parameter is set to 0.05. Increasing it will make the trend *more* flexible:


```R
# R
m <- prophet(df, changepoint.prior.scale = 0.5)
forecast <- predict(m, future)
plot(m, forecast)
```
```python
# Python
m = Prophet(changepoint_prior_scale=0.5)
forecast = m.fit(df).predict(future)
fig = m.plot(forecast)
```

![png](/prophet/static/trend_changepoints_files/trend_changepoints_13_0.png)


Decreasing it will make the trend *less* flexible:


```R
# R
m <- prophet(df, changepoint.prior.scale = 0.001)
forecast <- predict(m, future)
plot(m, forecast)
```
```python
# Python
m = Prophet(changepoint_prior_scale=0.001)
forecast = m.fit(df).predict(future)
fig = m.plot(forecast)
```

![png](/prophet/static/trend_changepoints_files/trend_changepoints_16_0.png)


When visualizing the forecast, this parameter can be adjusted as needed if the trend seems to be over- or under-fit. In the fully-automated setting, see the documentation on cross validation for recommendations on how this parameter can be tuned.


<a id="specifying-the-locations-of-the-changepoints"> </a>

### Specifying the locations of the changepoints


If you wish, rather than using automatic changepoint detection you can manually specify the locations of potential changepoints with the `changepoints` argument. Slope changes will then be allowed only at these points, with the same sparse regularization as before. One could, for instance, create a grid of points as is done automatically, but then augment that grid with some specific dates that are known to be likely to have changes. As another example, the changepoints could be entirely limited to a small set of dates, as is done here:


```R
# R
m <- prophet(df, changepoints = c('2014-01-01'))
forecast <- predict(m, future)
plot(m, forecast)
```
```python
# Python
m = Prophet(changepoints=['2014-01-01'])
forecast = m.fit(df).predict(future)
fig = m.plot(forecast)
```

![png](/prophet/static/trend_changepoints_files/trend_changepoints_21_0.png)




================================================
FILE: docs/_docs/uncertainty_intervals.md
================================================
---
layout: docs
docid: "uncertainty_intervals"
title: "Uncertainty Intervals"
permalink: /docs/uncertainty_intervals.html
subsections:
  - title: Uncertainty in the trend
    id: uncertainty-in-the-trend
  - title: Uncertainty in seasonality
    id: uncertainty-in-seasonality
---
By default Prophet will return uncertainty intervals for the forecast `yhat`. There are several important assumptions behind these uncertainty intervals.



There are three sources of uncertainty in the forecast: uncertainty in the trend, uncertainty in the seasonality estimates, and additional observation noise.



<a id="uncertainty-in-the-trend"> </a>

### Uncertainty in the trend

The biggest source of uncertainty in the forecast is the potential for future trend changes. The time series we have seen already in this documentation show clear trend changes in the history. Prophet is able to detect and fit these, but what trend changes should we expect moving forward? It's impossible to know for sure, so we do the most reasonable thing we can, and we assume that the *future will see similar trend changes as the history*. In particular, we assume that the average frequency and magnitude of trend changes in the future will be the same as that which we observe in the history. We project these trend changes forward and by computing their distribution we obtain uncertainty intervals.



One property of this way of measuring uncertainty is that allowing higher flexibility in the rate, by increasing `changepoint_prior_scale`, will increase the forecast uncertainty. This is because if we model more rate changes in the history then we will expect more in the future, and makes the uncertainty intervals a useful indicator of overfitting.



The width of the uncertainty intervals (by default 80%) can be set using the parameter `interval_width`:


```R
# R
m <- prophet(df, interval.width = 0.95)
forecast <- predict(m, future)
```
```python
# Python
forecast = Prophet(interval_width=0.95).fit(df).predict(future)
```
Again, these intervals assume that the future will see the same frequency and magnitude of rate changes as the past. This assumption is probably not true, so you should not expect to get accurate coverage on these uncertainty intervals.



<a id="uncertainty-in-seasonality"> </a>

### Uncertainty in seasonality

By default Prophet will only return uncertainty in the trend and observation noise. To get uncertainty in seasonality, you must do full Bayesian sampling. This is done using the parameter `mcmc.samples` (which defaults to 0). We do this here for the first six months of the Peyton Manning data from the Quickstart:


```R
# R
m <- prophet(df, mcmc.samples = 300)
forecast <- predict(m, future)
```
```python
# Python
m = Prophet(mcmc_samples=300)
forecast = m.fit(df, show_progress=False).predict(future)
```
This replaces the typical MAP estimation with MCMC sampling, and can take much longer depending on how many observations there are - expect several minutes instead of several seconds. If you do full sampling, then you will see the uncertainty in seasonal components when you plot them:


```R
# R
prophet_plot_components(m, forecast)
```
```python
# Python
fig = m.plot_components(forecast)
```

![png](/prophet/static/uncertainty_intervals_files/uncertainty_intervals_11_0.png)


You can access the raw posterior predictive samples in Python using the method `m.predictive_samples(future)`, or in R using the function `predictive_samples(m, future)`.



